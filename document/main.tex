% === Revtex Declaration ===
\documentclass[aps, 10pt, english, twoside, pra, nofootinbib, longbibliography]{revtex4-1}

\usepackage{silence}
\WarningFilter{revtex4-1}{Repair the float}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
% === All of the Packages I use frequently ===
\usepackage{../packages/shared}

% === Drafting Declarations ===
\draftprofile[TC Fraser]{TC}{Red}
\draftprofile[Elie Wolfe]{E}{Green}

% === Numbering For amsthm theorem environments ===
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% === Commonly re-occurring symbols ===
\newcommand{\hgraph}{\mathcal{H}}
\newcommand{\graph}{\mathcal{G}}
\newcommand{\nodes}{\mathcal{N}}
\newcommand{\weights}{\mathcal{W}}
\newcommand{\edges}{\mathcal{E}}
\newcommand{\trans}{\mathcal{T}}
\newcommand{\ind}{\mathcal{I}}
\newcommand{\perm}{\Omega}
\newcommand{\Hilb}{\mathcal{H}}
\newcommand{\ancestralindep}{\perp}
\newcommand{\valprod}{\times}
\newcommand{\bigvalprod}{\prod}
\newcommand{\com}{\mathrel{\Yright\hspace{-0.2em}\Yleft}}
\newcommand{\ext}{\mathrel{\Yleft}}
\newcommand{\res}{\mathrel{\Yright}}
\newcommand{\gelem}{\varphi}
\newcommand{\action}[1]{\gelem\bs{#1}}
\newcommand{\mscenario}{\mathcal{M}}
\newcommand{\jointvar}{\mathcal{J}}
\newcommand{\mext}[1]{E_{#1}}
\newcommand{\mul}[1]{\Gamma_{#1}}
\newcommand{\isext}[2]{\delta_{#1 \ext #2}}
\newcommand{\ts}{\mathcal{T}}
% When I introduce a new term or definition
\newcommand{\term}[1]{\textcolor{Mahogany}{\textbf{#1}}}

% === Mathematical operators that are denoted with words ===
\newcommand{\setoperator}[1]{%
\expandafter\DeclareDocumentCommand\csname#1\endcsname{O{} O{} m}{{\mathsf{#1}_{##1}^{##2}}\br{##3}}%
}

\setoperator{Pa} % Parents of a node
\setoperator{Ch} % Children of a node
\setoperator{An} % Ancestry of a node
\setoperator{Sub} % subgraph
\setoperator{AnSub} % Ancestral subgraph
\setoperator{Ext} % Extension of outcomes
\setoperator{Com} % Compatible equivalence relation
\setoperator{Nec} % Necessary nodes
\setoperator{UnNec} % Unnecessary nodes
\setoperator{Inj} % injectable sets
\setoperator{ImInj} % images of the injectable sets
\setoperator{PreInj} % Pre-injectable sets
\setoperator{ImPreInj} % images of the Pre-injectable sets
\setoperator{Aut} % graph automorphism
\setoperator{Orb} % Group Orbits
\setoperator{Perm} % Permutation group
\setoperator{Out} % Outcome space
\setoperator{Dom} % Domian

\newcommand{\outc}[1]{o\bs{#1}} % Fritz's BBTII notation for observed outcomes
\DeclareDocumentCommand{\prob}{O{} O{}}{\ifthenelse{\equal{#1}{}}{P}{P_{#1}}\ifthenelse{\equal{#2}{}}{}{\br{#2}}} % probability distribution
\DeclareDocumentCommand{\probvec}{O{} O{}}{\ifthenelse{\equal{#1}{}}{\mathcal{P}}{\mathcal{P}_{#1}}\ifthenelse{\equal{#2}{}}{}{\br{#2}}} % probability distribution vector

% === To allow for labeling of rows and columns of a matrix ===
\usepackage{kbordermatrix}
\renewcommand{\kbldelim}{(} % Left delimiter
\renewcommand{\kbrdelim}{)} % Right delimiter

% === Causal structure formatting for tikz ===
\input{../figures/causal_structures/formatting}

\begin{document}
    \title{Inequalities Witnessing Quantum Incompatibility in The Triangle Scenario}
    \author{Thomas C. Fraser}
    \email{tcfraser@tcfraser.com}
    \affiliation{Perimeter Institute for Theoretical Physics, Waterloo, Ontario, Canada \\ University of Waterloo, Waterloo, Ontario, Canada}
    % \author{Elie Wolfe}
    % \email{ewolfe@perimeter@institute.ca}
    % \affiliation{Perimeter Institute for Theoretical Physics, Waterloo, Ontario, Canada}
    \date{\today}
    \begin{abstract}
        Causal inference aims to identify which observable distributions are compatible with a given causal structure. Compatibility inequalities for the triangle scenario have been obtained and demonstrated to be violated by quantum configurations. Obtaining these inequalities has lead to the development of new computational techniques suitable for large causal structures with numerous outcomes. Numerical optimizations against these inequalities have found new quantum configurations that are incompatible with the triangle scenario, potentially suggesting new quantum resources qualitatively different from those known previously. \todo[TC]{Write a better abstract.}
    \end{abstract}
    \maketitle
    \tableofcontents

    \section{Introduction}
    \todo[TC]{Discuss why the marginal problem is important.}
    \section{Definitions \& Notation}
    \comment[TC]{This entire section should be integrated into the body when logical to do so.}
    \begin{definition}
        Following common notation used in \cite{Fritz_2011}, we use the notation $\bs{k} = \bc{1, \ldots, k}$ to be a finite index set. Moreover, when referencing a finite set with $k$ elements, we can write,
        \[ A_{\bs{k}} \defined \bc{A_i}_{i \in \bs{k}} = \bc{A_1, \ldots, A_k} \]
    \end{definition}
    \begin{definition}
        \label{def:outcome_space}
        Each random variable $v$ has a set of possible outcomes which are labeled by \term{outcome labels} $O_v$. The set of all outcomes themselves is its \term{outcome space} and is represented by the set of all functions from $v$ to $O_v$\footnote{Using functional language for outcome events is necessary for a sheaf-theoretic treatment of contextuality \cite{Abramsky_2011}.},
        \[ \Out{v} \defined \bc{f : \bc{v} \to O_v} \]
        Each of these functions has a singular domain ($v$) and thus can be concisely written in a compact form $\bc{v \mapsto o}$ where $o \in O_v$ represents some generic outcome label. Each function $f$ should be interpreted as the \textit{event} that $v$ obtains the outcome $o$, as well as the function that takes $v$ to $o$ $\bc{v \mapsto f\br{v} = o}$. Evidently $\Out{v}$ is isomorphic to $O_v$, and in many cases the two can be considered equivalent; however we make this particular semantic distinction now in preparation of \cref{sec:symmetry}.
    \end{definition}

    \begin{definition}
        % This notation generalizes to more than one variable. Letting $V = \bc{v_1, v_2}$, we define $\Out{V}$ to be the set of all joint valuations of $v_1$ and $v_2$.
        % Where the valuation product between outcome spaces is defined as follows,\footnote{At this level, a valuation product behaves analogously to a Cartesian product. However in \cref{def:extendable_set}, we define the a valuation product between outcomes and outcomes spaces.}
        % \[ \Out{V} = \Out{\bc{v_1, v_2}} \defined \bc{f : \bc{v_1, v_2} \to \bc{O_{v_1}, O_{v_2}} \mid f\br{v_i} \in O_{v_i}, i \in \bc{1,2}}  \]
        As a natural generalization, when considering a larger set of random variables $V = \bc{v_i}_{i \in \bs{k}}$ we define the joint outcome space in an analogous way,
        \[ \Out{V} \defined \bc{f: \bc{v_i}_{i \in \bs{k}} \to \bc{O_{v_i}}_{i \in \bs{k}} \mid \forall i \in \bs{k} : f\br{v_i} \in O_{v_i} } \eq \label{eq:outcome_space}\]
        Each outcome $f$ can be compactly represented as a set of mappings over each element of $V$.
        \[ f = \bc{v_1 \mapsto f\br{v_1}, \ldots, v_k \mapsto f\br{v_k}} = \bc{v_i\mapsto f\br{v_i}}_{i \in \bs{k}} \]
        The domain of $f$ refers to the set of random variables it valuates and is denoted as $\Dom{f}$. If $f \in \Out{V}$, then $\Dom{f} = V$.
        % Where the joint outcome labels are elements of a Cartesian product,
        % \[ O_V \defined O_{v_1} \times \cdots \times O_{v_{\abs{V}}}  \]

        % And a generic, unspecified joint outcome in $\Out{V}$ is written as $\outc{V}$ in a similar fashion to \cref{def:outcome_space} and is represented by a tuple of individual outcomes.
        % \[ \outc{V} \defined \br{\outc{v_1}, \ldots, \outc{v_{k}}} = \br{\outc{v}}_{v \in V} \eq \label{eq:outcomespace}\]
        % It is important to note that the ordering of events in \cref{eq:outcomespace} is irrelevant.
         % The definition of a valuation product can also be extended to outcomes themselves. Let $A$ and $B$ be sets of random variables and let $\outc{A} \in \Out{A}$ and $\outc{B} \in \Out{B}$. If $$
    \end{definition}

    \begin{example}
        Interpreting $f \in \Out{V}$ as an \textit{event} or \textit{outcome} on $V$, a probability distribution $\prob[V]$ over $V$ will assign a non-negative probability $\prob[V][f]$ to $f$. Let $V = \bc{v_1, v_2}$ with each $v \in V$ having $3$ distinct outcomes $O_v = \bc{0,1,2}$. Furthermore let $f \in \Out{V}$ be the outcome $\br{v_1 \mapsto 2, v_2 \mapsto 1}$. The following class of notations will be considered interchangeable,
        \[ \prob[V][f] = \prob[v_1, v_2][f] = \prob[][v_1 \mapsto f_1\br{v_1}, v_2 \mapsto f_2\br{v_2}] = \prob[][v_1 \mapsto 2, v_2 \mapsto 1] = \prob[v_1v_2][21] \]
        Each of which should be read as ``the probability that $v_1$ gets outcome $2$ AND $v_2$ gets outcome $1$''.
    \end{example}

    \begin{remark}
        \label{rem:domain_dist}
        In each of the above notations, $\prob[V]$ always represents a probability distribution over the \textit{domain} of $f \in \Out{V}$. In cases where the subscript is omitted, it should be assumed to be over the domain of $f$.
        \[ \prob[V][f] = \prob[\Dom{f}][f] = \prob[][f] \]
    \end{remark}

    \begin{definition}
        \label{def:restriction}
        Let $V = \bc{v_1, \ldots, v_k}$ be a set of random variables and $W \subseteq V$ be a subset of $V$. Moreover, let $f \in \Out{V}$ be a joint outcome over $V$; assigning a unique label to each variable in $V$. The \term{restriction of $f$ onto $W$} (denoted $f|_{W} \in \Out{W}$) is the outcome of $\Out{W}$ that \textit{agrees} with each of $f$'s assignments for variables in $W$.
        \[ \forall v \in W : f\br{v} = f|_{W}\br{v} \]
    \end{definition}

    \begin{definition}
        \label{def:outcome_compatible}
        Consider two sets of random variables $V$ and $W$ and two specific outcomes $f_V \in \Out{V}$ and $f_W \in \Out{W}$. The outcomes $f_V$ and $f_W$ are said to be \term{compatible} if they agree on their valuations of $V \cap W$. Compatibility will be denoted with `$\com$'.
        \[ f_V \com f_W \iff {f_V}|_{V\cap W} = {f_W}|_{V\cap W} \]
        Compatibility is a reflexive and symmetric, but \textit{not} transitive. There are two cases when compatibility is transitive; whenever $V \cap W = \emptyset$ or $V = W$. Whenever $V \cap W = \emptyset$, $f_V$ and $f_W$ can be said to be \textit{trivially} compatible. Moreover if $V = W$, compatibility becomes ordinary equivalence,
        \[ V = W \quad \textrm{and} \quad f_V \com f_W \iff f_V = f_W \]
    \end{definition}

    \begin{definition}
        \label{def:extendable}
        Let $f_V \in \Out{V}$ and $f_W \in \Out{W}$ be outcomes of $V$ and $W$ respectively as in \cref{def:outcome_compatible}. If $f_V$ and $f_W$ are compatible, and $W \subseteq V$ then $f_W$ is said to be \term{extendable} to $f_V$ and is denoted `$\ext$'.
        \[ f_W \ext f_V \iff f_W \com f_V \quad \textrm{and} \quad W \subseteq V \iff {f_W} = {f_V}|_{W} \]
        The dual language is also appropriate: $f_W$ is a \term{restriction} of $f_V$ (denoted $f_V \res f_W$) if and only if $f_W$ is extendable to $f_V$. If $f_W \ext f_V$ then there exists a unique element $f_{V \setminus W} \in \Out{V \setminus W}$ such that $f_V$ can be reconstructed by gluing together $f_{V \setminus W}$ and $f_W$.
        \[ \forall v \in V : f_V\br{v} = \begin{cases}
            f_W\br{v} & v \in W \\
            f_{V\setminus W}\br{v} & v \in V \setminus W
        \end{cases}  \]
    \end{definition}

    \begin{remark}
        The notation `$g \com h$' is used to indicate compatibility between $g \in \Out{V}$ and $h \in \Out{W}$ because of the following observation; $g$ and $h$ are compatible if and only if extensions of some outcome $f \in \Out{V \cap W}$.
        \[ g \com h \iff \exists f \in \Out{V \cap W} \mid g \res f \ext h \]
        In fact, such an $f$ is uniquely defined,
        \[ \forall v \in V \cap W : f\br{v} = g\br{v} = g\br{v} \]
    \end{remark}

    \begin{definition}
        \label{def:extendable_set}
        The set of all extendable outcomes of $f \in \Out{W}$ into $V$ is called the \term{extendable set of $f$ into $V$} and can be written as,
        \[ \Ext[V]{f} \defined \bc{g \in \Out{V} \mid f \ext g} \subseteq \Out{V} \]
        The extendable set of $f$ into $V$ is the subset of outcomes in $\Out{V}$ that \textit{agree} with $f$ about valuations for variables in $W$.
    \end{definition}


    \begin{example}
        Let $W = \bc{a, b}$ and $V = \bc{a, b, c}$ be two sets of random variables. Clearly $W \subseteq V$; a prerequisite for extendability. Further impose that the outcome labels for each variable are the same: $O_a = O_b = O_c = \bc{0,1,2}$. As an example, let $f = \bc{a \mapsto f\br{a} = 1, b \mapsto f\br{b} = 2} \in \Out{W}$; $f$ is extendable to the outcome $g = \bc{a \mapsto g\br{a} = 1, b \mapsto g\br{b} = 2, c \mapsto g\br{c} = 1} \in \Out{V}$, and the extendable set of $f$ into $V$ is,
        \[ \Ext[V]{f} = \Ext[a,b,c]{a \mapsto 1, b \mapsto 2} = \bc{\bc{a \mapsto 1, b \mapsto 2, c \mapsto 0}, \bc{a \mapsto 1, b \mapsto 2, c \mapsto 1}, \bc{a \mapsto 1, b \mapsto 2, c \mapsto 2}} \]
    \end{example}

    % \begin{definition}
    %     Two outcomes $\outc{V_1}$ and $\outc{V_2}$ over two distinct sets of random variables $V_1$ and $V_2$ are called \term{accordant} or \textit{compatible} if they are mutually realizable. In terms of extendable sets, $\outc{V_1}$ and $\outc{V_2}$ are accordant if there exists an outcome $\outc{V_1 \cup V_2} \in O_{V_1 \cup V_2}$ such that,
    %     \[ \outc{V_1}, \outc{V_2} \subseteq \outc{V_1 \cup V_2}  \]
    %     Accordance between two outcomes is the \textit{opposite} of exclusive.
    % \end{definition}

    \begin{definition}
        \label{def:graph}
        A \term{graph} is an ordered tuple $\br{\nodes, \edges}$ of \textit{nodes} and \textit{edges} respectively where the nodes can represent any object and the edges are pairs of nodes. For convenience of notation, one defines an index set over the nodes denoted $\ind_\nodes$.
        \[ \nodes = \bc{n_i \mid i \in \ind_\nodes} \quad \edges = \bc{\bc{n_j, n_k} \mid j,k \in \ind_\nodes} \]
    \end{definition}

    \begin{definition}
        \label{def:directed_graph}
        A \term{directed graph} $\graph$ is an ordered tuple $\br{\nodes, \edges}$ of \textit{nodes} and \textit{edges} respectively where the nodes can represent any object and the edges are \textit{ordered} pairs of nodes. For convenience of notation, one defines an index set over the nodes denoted $\ind_\nodes$.
        \[ \nodes = \bc{n_i \mid i \in \ind_\nodes} \quad \edges = \bc{n_j \to n_k \mid j,k \in \ind_\nodes} \]
    \end{definition}

    \begin{definition}
        \label{def:graph_terms}
        The following definitions are common language in directed graph theory. Let $n, m \in \nodes$ be example nodes of the graph $\graph$.
        \begin{itemize}
            \item The \term{parents of a node}: $\Pa[\graph]{n} \defined \bc{m \mid m \to n}$
            \item The \term{children of a node}: $\Ch[\graph]{n} \defined \bc{m \mid n \to m}$
            \item The \term{ancestry of a node}: $\An[\graph]{n} \defined \bigcup_{i\in\mathbb{W}} \Pa[\graph][i]{n}$ where $\Pa[\graph][i]{n} \defined \Pa[\graph]{\Pa[\graph][i-1]{n}}$ and $\Pa[\graph][0]{n} = n$
        \end{itemize}
        All of these terms can be generalized to sets of nodes $N \subseteq \nodes$ through union over the elements,
        \begin{itemize}
            \item The \term{parents of a node set}: $\Pa[\graph]{N} \defined \bigcup_{n\in N}\Pa[\graph]{n}$
            \item The \term{children of a node set}: $\Ch[\graph]{N} \defined \bigcup_{n\in N}\Ch[\graph]{n}$
            \item The \term{ancestry of a node set}: $\An[\graph]{N} \defined \bigcup_{n\in N}\An[\graph]{n}$
        \end{itemize}
        Moreover, an \term{induced subgraph} of $\graph$ due to a set of nodes $N \subseteq \nodes$ is the graph composed of $N$ and all edges $e \in \edges$ of the original graph that are contained in $N$.
        \[ \Sub[\graph]{N} \defined \br{N, \bc{e_i \mid i \in \ind_\edges, e_i \subseteq N}}\]
        An \term{ancestral subgraph} of $\graph$ due to $N \subseteq \nodes$ is the induced subgraph due to the ancestry of $N$.
        \[ \AnSub[\graph]{N} \defined \Sub[\graph]{\An[\graph]{N}} \]
    \end{definition}

    \begin{definition}
        \label{def:dag}
        A \term{directed acyclic graph} or \term{DAG} $\graph$ is an directed graph \cref{def:directed_graph} with the additional property that no node $n$ is in its set of \term{ancestors}.
        \[ \forall n \in \nodes : n \notin \bigcup_{i\in\mathbb{N}} \Pa[\graph][i]{n}\]
        Notice the difference between using the natural numbers $\mathbb{N}$ to distinguish \textit{ancestors} from \textit{ancestry}.
    \end{definition}

    \begin{definition}
        \label{def:hypergraph}
        A \term{hypergraph} denoted $\hgraph$ is an ordered tuple $\br{\nodes, \edges}$ of \textit{nodes} and \textit{hyperedges} respectively where the nodes can represent any object and the hyperedges are \textit{subsets} of nodes. For convenience of notation, one defines an index set over the nodes and hyperedges of a hypergraph $\hgraph$ denoted $\ind_\nodes$ and $\ind_\edges$ respectively.
        \[ \hgraph = \br{\nodes, \edges} \quad \nodes = \bc{n_i \mid i \in \ind_\nodes} \quad \edges = \bc{e_i \mid i \in \ind_\edges, e_i \subseteq \nodes} \]
        Note that whenever the hyperedge or node index is arbitrary, it will be omitted. There is a dual correspondence between hyperedges $e \in \edges$ and nodes $n \in \nodes$ in a Hypergraph. A hyperedge $e$ is viewed as a set of nodes $\bc{n_i}$, and a node $n$ can be viewed as the set of hyperedges $\bc{e_i}$ that contain it.
    \end{definition}

    \begin{definition}
        \label{def:hgraph_trans}
        A \term{hypergraph transversal} (or edge hitting set) $\trans$ of a hypergraph $\hgraph$ is a set of nodes $\trans \subseteq \nodes$ that have non-empty intersections with every hyperedge in $\edges$.
        \[ \trans = \bc{n_i \in \nodes \mid i \in \ind_\trans } \quad \forall e \in \edges : \trans \cap e \neq \emptyset \]
    \end{definition}

    \begin{definition}
        A \term{necessary node} of a transversal $\trans$ is a node $n$ such that $\trans \setminus n$ is no longer a valid transversal. The set of all necessary nodes is denoted $\Nec{\trans}$,
        \[ \Nec{\trans} = \bc{n \in \trans \mid \exists e \in \edges : \br{\trans \setminus n} \cap e = \emptyset} \]
        An \term{unnecessary node} of a transversal $\trans$ is any node that is \textit{not} necessary. The set of all unnecessary nodes is denoted $\UnNec{\trans}$,
        \[ \UnNec{\trans} = \trans \setminus \Nec{\trans} \]
        A \term{minimal hypergraph transversal} $\trans$ is any valid transversal of $\hgraph$ where every node $n$ is \textit{necessary}.
        \[ \trans = \Nec{\trans} \]
    \end{definition}

    \begin{definition}
        A \term{weighted hypergraph} $\hgraph_\weights$ is a regular hypergraph satisfying \cref{def:hypergraph} equipped with a set of weights $\weights$ ascribed to each node such that a weighted hypergraph is written as a triplet $\br{\weights, \nodes, \edges}$.
        \[ \weights = \bc{w_i \mid i \in \ind_\nodes, w_i \in \R} \]
        One would say that a particular node $n_i$ carries weight $w_i$ for each $i \in \ind_\nodes$.
    \end{definition}

    \begin{definition}
        A \term{bounded transversal} of a weighted hypergraph $\hgraph_\weights$ is a transversal $\trans$ of the unweighted hypergraph $\hgraph$ and a real number $t$ (denoted $\trans_{\leq t}$) such that the sum of the node weights of the transversal is bounded by $t$.
        \[ \trans_{\leq t} = \bc{n_i \mid i \in \ind_\trans} \quad \text{s.t.} \sum_{j\in\ind_\trans} w_j \leq t \]
        One can definte analogous \term{(strictly) upper/lower bounded transversals} by considering modifications of the notation: $\trans_{< t}, \trans_{\geq t}, \trans_{> t}$.
    \end{definition}

    \begin{definition}
        A \term{causal structure} is simply a DAG with the extra classification of each node into one of two categories; the \term{latent nodes} and \term{observed nodes} denoted $\nodes_L$ and $\nodes_O$. The latent nodes correspond to random variables that are either hidden through some fundamental process or cannot/will not be measured. The observed nodes are random variables that are measurable. Every node is either latent or observed and no node is both:
        \[ \nodes_L \cap \nodes_O = \emptyset \qquad \nodes_L \cup \nodes_O = \nodes \]
    \end{definition}

    \begin{definition}
        The \term{product distribution} of two disjoint distributions $\prob[V]$ and $\prob[W]$ (where $V \cap W = \emptyset$) is denoted as usual with $\prob[V] \times \prob[W]$ and is defined as,
        \[ \forall f \in \Out{V \cup W} : \br{\prob[V] \times \prob[W]}\br{f} \defined \prob[V][f|_{V}]\cdot\prob[W][f|_{W}] \]
        A product distribution of $k$ mutually disjoint distributions is defined as,
        \[ \prod_{i\in\bs{k}} \prob[V_i] \defined \br{\prob[V_1] \times \cdots \times \prob[V_k]} \]
    \end{definition}

    \begin{definition}
        The \term{marginalization} of a distribution $\prob[V]$ to a distribution over $W \subseteq V$ is denoted $\sum_{V \setminus W} \prob[V]$ and is defined such that,
        \[ \forall f \in \Out{W} : \br{\sum_{V \setminus W} \prob[V]}\br{f} \defined \sum_{g \in \Ext[V]{f}} \prob[V][g] \]
    \end{definition}

    \subsection{Preliminary Definitions}

    Before discussing the present work, we

    \todo[TC]{How many definitions do I need to write??}

    \section{Compatibility, Contextuality and the Marginal Problem}
    \label{sec:comp_con_mp}
    In order to determine if a given marginal distribution $\prob[V]$ or set of marginal distributions $\bc{\prob[V_i]}_{i \in \bs{k}} \defined \bc{\prob[V_1], \ldots, \prob[V_k]}$ is compatible with a causal structure $\graph$, one should first formalize what is meant by \textit{compatible}.

    \begin{definition}
        A set of \term{causal parameters} for a particular causal structure $\graph$ is the specification of a conditional distribution for every node $n \in \nodes$ given it's parents in $\graph$.
        \[ \bc{\prob[n \mid \Pa[\graph]{n}] \mid n \in \nodes} \]
    \end{definition}
    \begin{definition}
        \label{def:compatible}
        A marginal distribution $\prob[V]$ is \term{compatible} with a causal structure $\graph$ (where it is assumed that $V \subseteq \nodes_O$) if there exists a \textit{choice} of causal parameters $\bc{\prob[n \mid \Pa[\graph]{n}] \mid n \in \nodes}$ such that $\prob[V]$ can be \textit{recovered} from the following series of operations:
        \begin{enumerate}
            \item First obtain a joint distribution over \textit{all} nodes of of the causal structure,
            \[ \prob[\nodes] = \prod_{n \in \nodes} \prob[n \mid \Pa[\graph]{n}] \]
            \item Then marginalize over the latent nodes of $\graph$,
            \[ \prob[\nodes_O] = \sum_{\nodes_L} \prob[\nodes] \]
            \item Finally marginalize over the observed nodes not in $V$ to obtain $\prob[V]$,
            \[ \prob[V] = \sum_{\nodes_O \setminus V} \prob[\nodes_O] \]
        \end{enumerate}
        A set of marginal distributions $\bc{\prob[V_{i}]}_{i \in \bs{k}}$ is compatible with $\graph$ if each distribution $\prob[V_{i}]$ can be made compatible by the \textit{same} choice of causal parameters.
        A distribution $\prob[V]$ or set of distributions $\bc{\prob[V_{i}]}_{i \in \bs{k}}$ is said to be \term{incompatible} with a causal structure if there \textit{does not exist} a set of causal parameters with the above mentioned property.

        \todo[TC]{Source this?}
    \end{definition}
    Operations 2 and 3 of \cref{def:compatible} are related to the \textit{marginal problem}.
    \begin{definition}
        \label{def:marginal_problem}
        \term{The Marginal Problem:} Given a set of distributions $\bc{\prob[V_{i}]}_{i \in \bs{k}} \defined \bc{\prob[V_1], \ldots, \prob[V_k]}$ where $V_i \subseteq \jointvar$ for some set of random variables $\jointvar$, does there exist a joint distribution $\prob[\jointvar]$ such that each given distribution $\prob[V_i]$ can be obtained from marginalizing $\prob[\jointvar]$?
        \[ \forall i \in \bs{k} : \prob[V_i] = \sum_{\jointvar \setminus V_i} \prob[\jointvar] \]
        Typically (although not strictly necessary\footnote{Appending any additional variables to $\jointvar$ will not affect the existence of a joint distribution.}), $\jointvar$ is taken to mean the union of all $V_i$'s.
        \[ \jointvar = V_1 \cup \cdots \cup V_k = \bigcup_{i\in\bs{k}} V_i \eq \label{eq:marginals_union} \]
    \end{definition}

    \todo[TC]{Mention polytope perspective and existing methods}
    \begin{definition}
        \label{def:marginal_model}
        A reoccurring motif of these discussions will be the set of distributions $\bc{\prob[V_{i}]}_{i \in \bs{k}} \defined \bc{\prob[V_1], \ldots, \prob[V_k]}$ mentioned in \cref{def:marginal_problem}. In agreement with \cite{Fritz_2011} we call the set of subsets $\mscenario \defined \bc{V_1, \ldots, V_k}$ the marginal contexts or the \term{maximal marginal scenario}\footnote{Rigorously speaking as defined by \cite{Fritz_2011}, a marginal scenario forms an \textit{abstract simplicial complex} where it is required that $V' \in \mscenario$ whenever $V' \subseteq V_i$ for some $i \in \bs{k}$. ``Maximal'' refers to the restriction that $\forall i,j : V_i \not \subseteq V_j$.}. An individual $V_i \in \mscenario$ is a \term{marginal context}. The union of all contexts is denoted $\jointvar$ and is defined exactly as in \cref{eq:marginals_union}.
        Moreover, we will call the set of distributions a \term{marginal model} and denote it $\prob^{\mscenario}$ provided that they are \textit{compatible} \todo[TC]{Mention quantum non-signalling and maybe $k \geq 3$}:
        \[ \forall i \neq j \text{ if } V_i \cap V_j \neq \emptyset \text{ then } \sum_{V_i \setminus V_j} \prob[V_i] = \sum_{V_j \setminus V_i} \prob[V_j]  \]
        Following definition 2.3 in \cite{Fritz_2011}, a marginal model is said to be \term{contextual} if it \textit{does not} admit a joint distribution $\prob[\jointvar]$ and \term{non-contextual} otherwise.

        In addition, we elect to define the \term{marginal outcomes} $\Out{\mscenario}$ to be the set of all outcomes belonging to outcomes $\Out{V_i}$ of the marginal context\footnote{Here `$\coprod$' refers to the \textit{disjoint union} of outcome contexts; when $i \neq j$, $\Out{V_i} \cap \Out{V_j} = \emptyset$ since $V_i \neq V_j$.}.
        \[ \Out{\mscenario} \defined \coprod_{i \in \bs{k}} \Out{V_i} \]
        % By an intended abuse of notation, we say that a particular outcome is an ``element'' of $O_{\mscenario}$ if it is an element of $O_{V_i}$ for some $i \in \bs{k}$.
        % \[ \outc{V_i} \in O_{\mscenario} \]
        Each marginal outcome $m \in \Out{\mscenario}$ belongs to a unique marginal outcome space $\Out{V_i}$. To make reference to the marginal context $V_i$ associated with $m$, recall \cref{rem:domain_dist}: the context for $m$ is precisely the domain of $m$ ($V_i = \Dom{m}$). Note that $\Out{\mscenario}$ is a slight abuse of notation; $\Out{\mscenario}$ is \textit{not} a valid outcome space, analogous to the fact that $P^{\mscenario}$ is \textit{not} a probability distribution. Instead $\Out{\mscenario}$ is a \textit{flattened} collection of outcome spaces just as $P^{\mscenario}$ is a collection of distributions. The \term{joint outcomes} $\Out{\jointvar}$ are defined in the usual fashion of \cref{def:outcome_space}.
    \end{definition}

    \begin{definition}
        A \term{causal compatibility inequality} $\mathcal{I}_{\mscenario}$ for a marginal scenario $\mscenario$ is a probabilistic inequality that is obeyed for every compatible marginal model $\prob^{\mscenario}$. Whenever the marginal scenario $\mscenario$ in question is evident by context, the subscript of $\mathcal{I}_{\mscenario}$ will be dropped leaving just $\mathcal{I}$.
    \end{definition}

    \todo[TC]{Discuss Compatibility, connection to cooperative games/resources, bell incompatibility?}
    \todo[TC]{Connection between contextuality and Compatibility via the marginal problem for causal parameters}
    \todo[TC]{Discuss what is meant by a `complete' solution to the marginal problem}
    \todo[TC]{Maybe define the possibilistic marginal problem for later}

    \section{The Triangle Scenario}
    \label{sec:triangle_scenario}
    \begin{figure}
    \begin{center}
        \begin{minipage}[b]{.48\textwidth}
            \centering
            \scalebox{1.0}{\input{../figures/causal_structures/triangle_scenario}}
            \caption{The casual structure of the Triangle Scenario. Three variables $A,B,C$ are observable and illustrated as triangles, while $X, Y, Z$ are latent variables illustrated as circles.}
            \label{fig:triangle_scenario}
        \end{minipage}\hspace{0.04\textwidth}%
        \begin{minipage}[b]{.48\textwidth}
            \centering
            \scalebox{0.8}{\input{../figures/causal_structures/inflated_triangle_scenario}}
            \caption{An inflated causal structure of the Triangle Scenario \cref{fig:triangle_scenario}.}
            \label{fig:inflated_triangle_scenario}
        \end{minipage}
    \end{center}
    \end{figure}

    The triangle scenario $\ts$ is a causal structure composed of $3$ parties $A, B, C$ arranged in a triangular configuration while pair-wise sharing latent variables $X, Y, Z$. It has that has been studied extensively in existing literature (see \cite[Fig. 1]{Steudel_2010}, \cite[Fig. 6]{Chaves_2014}, \cite[Fig. 8]{Branciard_2012}, \cite[Fig. 8, Appendix E]{Henson_2014}, \cite[Fig. 3]{Fritz_2012}, \cite[Fig. 1]{Inflation}, $\ldots$) and is reproduced here in \cref{fig:triangle_scenario} for convenience. At the time of publication for \cite{Branciard_2012} it was noted that characterizing locality in $\ts$ remained an open problem and that identifying locality constraints in this configuration seemed challenging. In \cite{Henson_2014}, $\ts$ was classified as an ``interesting'' causal structure.\footnote{Indicating that conditional independence relations are not a sufficient characterization of compatibility for $\ts$ \cite{Henson_2014}. Note that in $\ts$ there are \textit{zero} conditional independence relations over the observable nodes $A, B, C$.} Additionally $\ts$ is presented along with a family of entropic inequalities that constrain the set of compatible distributions \cite{Henson_2014}. Furthermore in \cite{Fritz_2012}, Fritz demonstrated that $\ts$ is the \textit{smallest} correlation scenario in which their exists quantum incompatible distributions by explicitly mentioning one (which we elect to call the \textit{Fritz distribution} in \cref{sec:fritz_dist}) and poses an important question asking if other incompatible quantum distributions exist. Finally in \cite{Inflation}, Wolfe et. al. make use of $\ts$ along with the Bell Scenario to demonstrate a novel technique for causal inference called the \textit{inflation technique} (summarized in \cref{sec:summary_inflation}). Wolfe et. al. validate the power and applicability of the inflation technique by proving incompatibility between $\ts$ and the $W$-type distribution; a quantum non-accessible distribution whose incompatibility is not witness-able by \textit{any} entropic inequalities or any other known constraints \cite{Inflation}. Moreover, a family of polynomial, $2$-outcome, compatibility inequalities are explicitly derived for an exemplary inflation of $\ts$ and $37$ non-trivial representatives are printed in \cite{Inflation}.

    Identifying quantum-accessible distributions that are incompatible with the triangle scenario is of particular importance as it \todo[TC]{Discuss why we are doing this.}

    As a preliminary search for quantum incompatibility in $\ts$, we performed numerical optimizations using bipartite qubit states and $2$-outcome POVM measurements (see \cref{sec:param_quantum_states} for details) against the $37$ compatibility inequalities printed in \cite{Inflation} as well as the entropic inequalities presented by \cite{Henson_2014}. Unfortunately, none of these inequalities could be violated. These early results suggest, although do not prove, that quantum non-locality is not present in $\ts$ for \textit{two-outcome} measurements. We make \textit{no claim} that this is a universal truth and simply take it as motivation to explore incompatibility in $\ts$ for a larger number of outcomes.

    Specifically, we make use of the inflation technique for an inflated triangle scenario $\ts'$ depicted in \cref{fig:inflated_triangle_scenario} and cast the \textit{causal compatibility} problem as a \textit{marginal problem}. We develop a new technique which we call the \textit{extendable covering procedure (ECP)} to find a complete solution to the marginal problem that is suitable for large causal structures where the observable nodes are equipped with numerous outcomes. We use this technique to find numerous compatibility inequalities, some of which witness the incompatibility of the Fritz distribution. Before discussing the ECP, the Fritz distribution will be re-defined explicitly here, followed by a quick summary of the inflation technique of \cite{Inflation}. Following these summaries of existing work, we present the ECP and its connection to Hardy-type paradoxes \cite{Inflation,Liang_2011,Mansfield_2012}, certificate inequalities and logical Bell inequalities \cite{Abramsky_2011}. Moreover, the ECP can be adapted to derive symmetric inequalities or inequalities bounding any subspace of marginal models. To conclude, we subject our found inequalities to non-linear optimizations and discuss the implications of our results.

    \section{The Fritz Distribution}
    \label{sec:fritz_dist}
    The \term{Fritz distribution} $P_F$ is a quantum-accessible distribution known to be incompatible with the Triangle Scenario \cite{Fritz_2012}. Explicitly, $P_F$ is a three-party ($A,B,C$), four-outcome ($1,2,3,4$) distribution that has form as follows:
    \begin{align*}
    \eq \label{eq:fritz_dist}
    \begin{split}
    \prob[F][111] = \prob[F][221] = \prob[F][412] = \prob[F][322] = \prob[F][233] = \prob[F][143] = \prob[F][344] = \prob[F][434] &= \f{1}{32}\br{2 + \sqrt{2}} \\
    \prob[F][121] = \prob[F][211] = \prob[F][422] = \prob[F][312] = \prob[F][243] = \prob[F][133] = \prob[F][334] = \prob[F][444] &= \f{1}{32}\br{2 - \sqrt{2}}
    \end{split}
    \end{align*}
    Here the notation $\prob[F][abc] = \prob[ABC][abc] = \prob[][A=a,B=b,C=c]$ is used. The Fritz distribution $\prob[F]$ can be realized with the following quantum configuration:
    \begin{equation}
    \begin{gathered}
    \label{eq:fritz_quantum_realization}
    \rho_{AB} = \ket{\Psi^+}\bra{\Psi^+} \quad \rho_{BC} = \rho_{CA} = \ket{\Phi^+}\bra{\Phi^+} \\
    M_{A} = \bc{\ket{0\psi_0}\bra{0\psi_0}, \ket{0\psi_{\pi}}\bra{0\psi_{\pi}}, \ket{1\psi_{-\pi/2}}\bra{1\psi_{-\pi/2}}, \ket{1\psi_{\pi/2}}\bra{1\psi_{\pi/2}}} \\
    M_{B} = \bc{\ket{\psi_{\pi/4}0}\bra{\psi_{\pi/4}0}, \ket{\psi_{5\pi/4}0}\bra{\psi_{5\pi/4}0}, \ket{\psi_{3\pi/4}1}\bra{\psi_{3\pi/4}1}, \ket{\psi_{-\pi/4}1}\bra{\psi_{-\pi/4}1}} \\
    M_{C} = \bc{\ket{00}\bra{00}, \ket{01}\bra{01}, \ket{10}\bra{10}, \ket{11}\bra{11}}
    \end{gathered}
    \end{equation}
    Where for convenience of notation $\psi_x$ is used to denote the superposition,
    \[ \ket{\psi_x} = \f{1}{\sqrt{2}}\br{\ket{0} + e^{ix}\ket{1}} \]
    Additionally $\ket{\Psi^+} = \f{1}{\sqrt{2}}\br{\ket{01} + \ket{10}}$ and $\ket{\Phi^+} = \f{1}{\sqrt{2}}\br{\ket{00} + \ket{11}}$ are two maximally entangled Bell states.

    Fritz first proved it's incompatibility \cite{Fritz_2012} by showing $C$ acts a moderator to ensure measurement pseudo-settings for $A$ and $B$ are independent, satisfying non-broadcasting requirements for the standard Bell scenario. In fact, by coarse-graining outcomes for $A$ and $B$ and treating $C$ as a measurement-setting moderator, $P_F$ maximally violates the CHSH inequality. To illustrate this, begin with the CHSH inequality \cite{CHSH_Original},
    \[ \ba{AB|S_A=1, S_B=1} + \ba{AB|S_A=1, S_B=2} + \ba{AB|S_A=2, S_B=1} - \ba{AB|S_A=2, S_B=2} \leq 2 \eq \label{eq:CHSH}\]
    Where $\ba{AB|S_A=i, S_B=j}$ is the correlation between $A$ and $B$ given the measurement settings for $A$ ($B$) is $i$ ($j$) respectively. Next each of $C$'s outcomes become the conditioned settings in \cref{eq:CHSH},
    \[ \ba{AB|C=2} + \ba{AB|C=3} + \ba{AB|C=4} - \ba{AB|C=1} \leq 2 \eq \label{eq:chsh_c}\]
    Finally, specifying the two outcome coarse-graining as $\bc{1,2,3,4} \rightarrow \bc{\br{1,4}, \br{2,3}}$ gives a definition of correlation,
    \[ \ba{AB} \defined 2\bc{\prob[][11] + \prob[][14] + \prob[][41] + \prob[][44] + \prob[][22] + \prob[][23] + \prob[][32] + \prob[][33]} - 1 \]
    Which when applied to the Fritz distribution \cref{eq:fritz_dist} gives the following correlations:
    \begin{gather*}
    \ba{AB|C=2} = 2 \bc{\f{1}{32}\bs{2\br{2+\sqrt{2}}+2\br{2-\sqrt{2}}}} - 1 = \f{\sqrt{2}}{2} \\
    \ba{AB|C=3} = \ba{AB|C=4} = \f{\sqrt{2}}{2} \quad \ba{AB|C=1} = -\f{\sqrt{2}}{2}
    \end{gather*}
    Which when applied to \cref{eq:chsh_c} gives the familiar Tsirelson violation of $2\sqrt{2} \not\leq 2$ \cite{Cirelson_1980}.

    Before continuing it is worth noting that \cref{eq:fritz_dist} is non-unique. Any distribution that is equal to \cref{eq:fritz_dist} via a permutation of outcomes or exchange of parties is also referred to as a Fritz distribution. Moreover, the quantum realization of \cref{eq:fritz_quantum_realization} is non-unique. In fact, \cref{eq:fritz_quantum_realization} is not the realization that Fritz originally had in mind. Nonetheless \cref{eq:fritz_dist} is taken as \textit{the} Fritz distribution for concreteness throughout this paper.

    In \cite{Fritz_2012}, Fritz posits the following question: \textit{Find an example of non-classical quantum correlations in $\ts$ together with a proof of its non-classicality which does not hinge on Bell’s Theorem.}



    \todo[TC]{Summarize Problem 2.17 in fritz BBT, make it more formal}

    \section{Summary of the Inflation Technique}
    \label{sec:summary_inflation}
    The causal inflation technique, first pioneered by Wolfe, Spekkens, and Fritz \cite{Inflation} and inspired by the \textit{do calculus} and \textit{twin networks} of Ref. \cite{Pearl_2009}, is a family of causal inference techniques that can be used to determine if a probability distribution is compatible or incompatible with a given causal structure. As a preliminary summary, the inflation technique begins by \textit{augmenting} a causal structure with additional copies of its nodes, producing an \textit{inflated} causal structure, and then exposes how causal inference tasks on the inflated causal structure can be used to make inferences on the original causal structure. Copies of the original nodes are distinguished by an additional subscript called the \term{copy-index}. For example node $A$ of \cref{fig:triangle_scenario} has copies $A_1, A_2, A_3, A_4$ in the inflation of \cref{fig:inflated_triangle_scenario}. Following the notions of Ref. \cite{Inflation}, all such copies are deemed equivalent via a \term{copy-index equivalence} relation denoted `$\sim$'. A copy-index is effectively arbitrary, so $A'$ will refer to an arbitrary inflated copy of $A$.
    \[ A \sim A_1 \sim A' \not\sim B \sim B_1 \sim B' \]
    We preemptively generalize the notion of copy-index equivalence to other mathematical objects like sets, graphs, and groups by saying that $X \sim Y$ if $X = Y$ upon removal of the copy-index. Equipped with the common graph-theoretic terminology and notation of \cref{def:graph_terms}, an inflation can be formally defined as follows:
    \begin{definition}
        An \term{inflation} of a causal structure $\graph$ is another causal structure $\graph'$ such that:
        \[ \forall n' \in \nodes': \AnSub[\graph']{n'} \sim \AnSub[\graph]{n} \eq \label{eq:inflation_defn}\]
    \end{definition}
    The motivation being that since the ancestry of each node $n'$ in $\graph'$ plays the \textit{exact same} role as the ancestry of its source copy $n$ in $\graph$, then every compatible joint distribution $\prob[\nodes]$ on $\graph$ can be used to \textit{create} compatible joint distributions on $\graph'$. To do this, first notice that every compatible joint distribution $\prob[\nodes]$ uniquely defines a set of causal parameters for $\graph$.
    \[ \forall n \in \nodes : \prob[n\mid \Pa[\graph]{n}] = \sum_{\nodes \setminus n} \prob[\nodes] \eq \label{eq:unique_causal_parameters}\]
    Now since \cref{eq:inflation_defn} enforces that $\Pa[\graph']{n'} \sim \Pa[\graph]{n}$, one can create a set of \term{inflated causal parameters} using \cref{eq:unique_causal_parameters},
    \[ \forall n' \in \nodes' : \prob[n'\mid \Pa[\graph']{n'}] \defined \prob[n\mid \Pa[\graph]{n}] \]
    Which in turn uniquely defines a compatible joint distribution $\prob[\nodes']$ on $\graph'$\footnote{Of course not all compatible joint distributions on $\nodes'$ are constructed in this way; all that is demonstrated is that all joint distributions constructed in this way are compatible.}.
    \[ \prob[\nodes'] = \prod_{n' \in \nodes'} \prob[n'\mid \Pa[\graph']{n'}] \]
    This is known as the \term{weak inflation lemma}; compatible joint distributions over $\nodes$ induce compatible joint distributions over $\nodes'$. Before generalizing to the inflation lemma, a careful observation needs to be made. For any pair of subsets of nodes $N \subseteq \nodes$ and $N' \subseteq \nodes'$ that are equivalent up to copy-index $N \sim N'$, if $\AnSub[\graph]{N} \sim \AnSub[\graph']{N'}$ then any compatible \textit{marginal} distribution $\prob[N]$ over $N$ induces a compatible marginal distribution $\prob[N']$ over $N'$. In fact since $N'$ contains no duplicate nodes up to copy index (simply because $N \sim N'$ and $N$ cannot contain duplicate nodes), $\prob[N] = \prob[N']$. We refer such sets $N'$ as the \term{injectable sets} of $\graph'$ and $N$ the \term{images of the injectable sets} of $\graph$.
    \begin{align*}
        \Inj[\graph]{\graph'} &\defined \bc{N' \subseteq \nodes' \mid \exists N \subseteq \nodes : N \sim N'} \\
        \ImInj[\graph]{\graph'} &\defined \bc{N \subseteq \nodes \mid \exists N' \subseteq \nodes' : N \sim N'}
    \end{align*}

    \begin{lemma}
        \label{lem:inflation}
        \term{The Inflation Lemma} Given a particular inflation $\graph'$ of $\graph$, if a marginal model $\bc{\prob[N] \mid N \in \ImInj[\graph]{\graph'}}$ is compatible with $\graph$ then all marginal models $\bc{\prob[N'] \mid N' \in \Inj[\graph]{\graph'}}$ are compatible with $\graph'$ provided that $\prob[N] = \prob[N']$ for all instances where $N \sim N'$.
        This is lemma 3 of \cite{Inflation}.
    \end{lemma}

    The inflation lemma is the most important result of the causal inflation technique \cite{Inflation}. The contrapositive version of \cref{lem:inflation} is a powerful tool for determining compatibility. Any compatibility constraint on marginal models $\bc{\prob[N'] \mid N' \in \Inj[\graph]{\graph'}}$ of the inflated causal structure $\graph'$ correspond to valid compatibility constraints on marginal models $\bc{\prob[N] \mid N \in \ImInj[\graph]{\graph'}}$ of the original causal structure. Corollary 5 of \cite{Inflation} proves this explicitly for incompatibility inequalities $\mathcal{I}$; which the remainder of this work focuses on.  Additionally, the inflation lemma holds when considering any subset of $\Inj[\graph]{\graph'}$ (analogously $\ImInj[\graph]{\graph'}$). Therefore, in situations where latent nodes are present in $\graph$, one only needs to consider injectable sets that are composed of \textit{observable nodes}.

    In this work, we obtain inequalities that constrain the set of injectable marginal models $\bc{\prob[N'] \mid N' \in \ImInj[\graph]{\graph'}}$ for the inflated Triangle Scenario of \cref{fig:triangle_scenario} by considering the marginal problem (see \cref{sec:comp_con_mp}) for the set of \term{maximally pre-injectable sets}\footnote{The set of all pre-injectable sets forms a topological \textit{simplicial conplex}. `Maximal' refers to the pre-injectable sets that are no proper subset of another.}. A pre-injectable set $V$ is a subset of $\nodes'$ that can be decomposed into the disjoint union of injectable sets $V = \coprod_i N'_i \mid N'_i \in \Inj[\graph]{\graph'}$ that are mutually \term{ancestrally independent},
    \[ \forall i,j : N'_i \ancestralindep N'_j \iff \An[\graph']{N'_i} \cap \An[\graph']{N'_j} = \emptyset  \]
    In doing so, any distribution over all nodes of a pre-injectable set $\prob[\cup_i N'_i]$ will factorize according to graphical $d$-separation conditions \cite{Pearl_2009},
    \[ \prob[V] = \prob[\cup_i N'_i] = \prod_{i}\prob[N'_i] \]
    This turns linear inequalities over the pre-injectable distributions into polynomial inequalities over the injectable distributions, allowing one to replace all such distributions with equivalent distributions over the original random variables of $\nodes$. $\PreInj[\graph]{\graph'}$ will denote the set of all pre-injectable sets.

    Focusing on the inflation $\ts'$ depicted in \cref{fig:inflated_triangle_scenario}, we obtained the injectable sets $\Inj[\ts]{\ts'}$ along with the maximally pre-injectable sets through a simple graphical procedure outlined in \cite{Inflation}.
    \begin{equation*}
        \eq \label{eq:pre-injectable_triangle_scenario}
        \begin{gathered}
            \textbf{Maximal Pre-injectable Sets} \\
            \bc{A_1, B_1, C_1, A_4, B_4, C_4} \\
            \bc{A_1, B_2, C_3, A_4, B_3, C_2} \\
            \bc{A_2, B_3, C_1, A_3, B_2, C_4} \\
            \bc{A_2, B_4, C_3, A_3, B_1, C_2} \\
            \bc{A_1, B_3, C_4} \\
            \bc{A_1, B_4, C_2} \\
            \bc{A_2, B_1, C_4} \\
            \bc{A_2, B_2, C_2} \\
            \bc{A_3, B_3, C_3} \\
            \bc{A_3, B_4, C_1} \\
            \bc{A_4, B_1, C_3} \\
            \bc{A_4, B_2, C_1}
        \end{gathered}
        \qquad
        \begin{gathered}
            \textbf{Ancestral Independences} \\
            \bc{A_1, B_1, C_1} \ancestralindep \bc{A_4, B_4, C_4} \\
            \bc{A_1, B_2, C_3} \ancestralindep \bc{A_4, B_3, C_2} \\
            \bc{A_2, B_3, C_1} \ancestralindep \bc{A_3, B_2, C_4} \\
            \bc{A_2, B_4, C_3} \ancestralindep \bc{A_3, B_1, C_2} \\
            \bc{A_1} \ancestralindep \bc{B_3} \ancestralindep \bc{C_4} \\
            \bc{A_1} \ancestralindep \bc{B_4} \ancestralindep \bc{C_2} \\
            \bc{A_2} \ancestralindep \bc{B_1} \ancestralindep \bc{C_4} \\
            \bc{A_2} \ancestralindep \bc{B_2} \ancestralindep \bc{C_2} \\
            \bc{A_3} \ancestralindep \bc{B_3} \ancestralindep \bc{C_3} \\
            \bc{A_3} \ancestralindep \bc{B_4} \ancestralindep \bc{C_1} \\
            \bc{A_4} \ancestralindep \bc{B_1} \ancestralindep \bc{C_3} \\
            \bc{A_4} \ancestralindep \bc{B_2} \ancestralindep \bc{C_1}
        \end{gathered}
    \end{equation*}
    As can be counted, there are $12$ maximally pre-injectable sets $\ts'$ which will be indexed $1$ through $12$ in the order seen above ($\bc{V_1, \ldots, V_{12}}$). The maximally pre-injectable sets $\bc{V_i}_{i \in \bs{12}}$ form a maximal marginal scenario $\mscenario$ over the observable nodes of $\ts'$ when equipped with outcome labels. Using \cref{lem:inflation} inequalities $\mathcal{I}_{\PreInj[\ts]{\ts'}}$ constraining the set of contextual marginal models $\prob^{\mscenario}$ can be deflated into polynomial inequalities over $\mathcal{I}_{\ImInj[\ts]{\ts'}}$ bounding compatible marginal models over the images of the injectable sets in $\ts$.
    \[ \ImInj[\ts]{\ts'} = \bc{\bc{A, B, C}, \bc{A}, \bc{B}, \bc{C}} \]

    \section{Certificate Inequalities}
    \label{sec:certificate_inequalities}
    \subsection{Casting the Marginal Problem as a Linear Program}
    After obtaining the maximal pre-injectable sets associated with a particular inflation, one can write the marginal problem of \cref{def:marginal_problem} as a linear program. The key observation is that marginalization is a \textit{linear} operator that can be performed via a matrix multiplication. To do this, we will define the \textit{incidence matrix}.
    \begin{definition}
        The \term{incidence matrix} $M$ for a marginal scenario $\mscenario = \bc{V_1, \dots, V_k}$ is a bit-wise matrix where the columns are indexed by \textit{joint} outcomes $j \in \Out{\jointvar}$ and the rows are indexed by \textit{marginal} outcomes $m \in \Out{\mscenario}$. The entries of $M$ are populated whenever a row index is extendable to a column index.
        \[ M\bs{m, j} = \de_{m \ext j} \defined \begin{cases}
            1 & m \ext j \\
            0 & \text{otherwise}
        \end{cases} \]
        Where $\de_{m \ext j}$ is an \textit{extendability indicator}. The incidence matrix has $\abs{\Out{\jointvar}}$ columns and $\abs{\Out{\mscenario}} = \sum_{i\in \bs{k}} \abs{\Out{V_i}}$ rows. The number of non-zero entries of $M$ is a simple expression,
        \[ \sum_{i\in\bs{k}} \abs{\Out{V_i}}\abs{\Out{\jointvar \setminus V_i}} = \sum_{i\in \bs{k}} \abs{\Out{\jointvar}} = k\abs{\Out{\jointvar}} \]
        This is due to the fact that each marginal context contributes a single non-zero entry to each column of $M$, resulting in $k\abs{\Out{\jointvar}}$ total non-zero entries.
    \end{definition}
    \todo[TC]{Computationally Efficient generation?}

    To illustrate this concretely, consider the following example:
    \begin{example}
        \label{ex:marginal_matrix}
        Let $\jointvar$ be $3$ binary variables $\jointvar = \bc{a,b,c}$ and consider the marginal scenario $\mscenario = \bc{\bc{a,c}, \bc{b}}$. The incidence matrix becomes:
        \[ M = \kbordermatrix{
            (a,b,c) \:\: \mapsto & (0,0,0) & (0,0,1) & (0,1,0) & (0,1,1) & (1,0,0) & (1,0,1) & (1,1,0) & (1,1,1) \\
            (a\mapsto0, c\mapsto0) & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
            (a\mapsto0, c\mapsto1) & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 \\
            (a\mapsto1, c\mapsto0) & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 \\
            (a\mapsto1, c\mapsto1) & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 \\
            (b\mapsto0)            & 1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 \\
            (b\mapsto1)            & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 1
        } \]
    \end{example}

    \begin{example}
        The marginal problem for the inflated causal structure $\ts'$ depicted in \cref{fig:inflated_triangle_scenario} concerns itself with the pre-injectable marginal scenario $\mscenario = \PreInj[\ts]{\ts'} = \bc{V_1, \ldots, V_{12}}$. Equipping the observed nodes of $\ts'$ with $4$-element outcome labels, the incidence matrix $M$ has $16,896$ rows and $16,777,216$ columns.
        \begin{align*}
            &\text{\# Rows} = \abs{\Out{\mscenario}} = \sum_{i=1}^{12} \abs{\Out{V_i}} = \sum_{i=1}^{12} 4^{\abs{V_i}} = 4 \cdot 4^6 + 8 \cdot 4^3 = 16,896 \\
            &\text{\# Columns} = \abs{\Out{\jointvar}} = 4^{\abs{\jointvar}} = 4^{12} = 16,777,216
        \end{align*}
        With $12 \cdot 4^{12} = 201,326,592$ non-zero entries. For obvious reasons, no attempt is made to reproduce it here.
    \end{example}

    In order to describe how marginalization can be written as matrix multiplication $M \cdot x = b$, two more quantities need to be described:

    \begin{definition}
        The \term{joint distribution vector} $\probvec[\jointvar]$ for a probability distribution $\prob[\jointvar]$ is the vector whose entries are the positive, real-valued probabilities that $\prob[\jointvar]$ assigns to each joint outcome of $j \in \Out{\jointvar}$. $\probvec[\jointvar]$ shares the same indices as the \textit{column} indices of $M$.
        \[ \forall j \in \Out{\jointvar} : \probvec[\jointvar]^\intercal\bs{j} = \prob[\jointvar][j] \]
    \end{definition}
    \begin{definition}
        The \term{marginal distribution vector} $\probvec^{\mscenario}$ for a marginal model $\prob^{\mscenario} = \bc{P_{V_1}, \ldots, P_{V_k}}$ is the vector whose entries are probabilities over the set of marginal outcomes $\Out{\mscenario}$. $\probvec^{\mscenario}$ shares the same indices as the \textit{row} indices of $M$.
        \[ \forall m \in \Out{\mscenario} : {\probvec^{\mscenario}}^\intercal\bs{m} = \prob[\Dom{m}][m] \]
        Where $\Dom{m} \in \mscenario$ and $\prob[\Dom{m}] \in \prob^{\mscenario}$ as per \cref{rem:domain_dist}.
    \end{definition}
    The marginal and joint distribution vectors are related via the incidence matrix $M$. Given a joint distribution vector $\probvec[\jointvar]$ one can obtain the marginal distribution vector $\probvec^{\mscenario}$ by multiplying $M$ by $\probvec[\jointvar]$.
    \[ \probvec^{\mscenario} = M \cdot \probvec[\jointvar] \eq \label{eq:marginal_problem_matrix} \]

    \begin{remark}
        The particular ordering of the rows and columns of $M$ carries no importance, but it \textit{must} be consistent between $M$, $\probvec[\jointvar]$ and $\probvec^{\mscenario}$. Throughout this work, a \textit{canonical ordering} is used: the marginal contexts are written in the same order as listed in the marginal scenario $\mscenario$ and outcomes within a marginal context are enumerated in a \textit{raveling} order; exemplified in \cref{ex:marginal_matrix}.
    \end{remark}

    The marginal problem can now be rephrased in the language of the incidence matrix. Suppose one obtains a marginal distribution vector $\probvec^{\mscenario}$. The marginal problem becomes equivalent to the question: \textit{Does there exist a joint distribution vector $\probvec[\jointvar]$ such that \cref{eq:marginal_problem_matrix} holds?}

    \begin{definition}
        \label{def:marginal_linear_program}
        \term{The Marginal Linear Program} is the following linear program:
        \begin{alignat*}{2}
            & \text{minimize:} \quad&& \emptyset \cdot x\\
            & \text{subject to:} && x \succeq 0 \\
            & && M \cdot x = \probvec^{\mscenario}
        \end{alignat*}
        If this ``optimization''\footnote{``Optimization'' is presented in quotes here because the minimization objective is trivially always zero ($\emptyset$ denotes the null vector of all zero entries). The primal value of the linear program is of no interest, all that matters is its \textit{feasibility}.} is \textit{feasible}, then there exists a vector $x$ than can satisfy \cref{eq:marginal_problem_matrix} and is a valid joint distribution vector. Therefore feasibility implies that $\prob[\jointvar] = x$, solving the marginal problem with positive result. Moreover if the marginal linear program is \textit{infeasible}, then there \textit{does not} exist a joint distribution $\prob[\jointvar]$ over all random variables.
    \end{definition}
    \begin{definition}
        \label{def:dual_marginal_linear_program}
        \term{The Dual Marginal Linear Program} is the dual of \cref{def:marginal_linear_program} formulated via a procedure similar to \cite{Lahaie_2008}:
        \begin{alignat*}{2}
            & \text{minimize:} \quad&& y \cdot \probvec^{\mscenario}\\
            & \text{subject to:} && y \cdot M \succeq 0
        \end{alignat*}
        Where $y$ is a real valued vector with the same length as $\probvec^{\mscenario}$.
    \end{definition}
    \subsection{Infeasibility Certificates}
    \label{sec:infeasibility_certificates}
    The dual marginal linear program also provides an answer to the marginal problem. To prove this, first notice that the dual problem is \textit{never infeasible}; by choosing $y$ to be trivially the null vector $\emptyset$ of appropriate size, all constraints are satisfied. Secondly if $y \cdot M \succeq 0$ and $x \succeq 0$, then the following must hold if the primal is feasible:
    \[ y \cdot \probvec^{\mscenario} =  y \cdot M \cdot x \geq 0 \eq \label{eq:dual_marginal_ineq} \]
    Therefore the \textit{sign} of the dual value $d \defined \min \br{y \cdot \probvec^{\mscenario}}$ solves the marginal problem. If $d < 0$ then \cref{eq:dual_marginal_ineq} is violated and therefore the marginal problem has negative result. Likewise if $d$ satisfies \cref{eq:dual_marginal_ineq}, then a joint distribution $\prob[\jointvar]$ exists. Before continuing, an important observation needs to be made. If $d \geq 0$, then it is exactly $d = 0$, due to the existence of the trivial $y = \emptyset$. This observation is an instance of the \textit{Complementary Slackness Property} of \cite{Bradley_1977}. \comment[TC]{Is this really the CSP?} Moreover, if $d < 0$, then it is unbounded $d = -\inf$. This latter point becomes clear upon recognizing that for any $y$ such that $d < 0$, another $y'$ can be constructed by multiplying $y$ by a real constant $\al$ greater than one such that,
    \[ y' = \al y \mid \al > 1 \implies d' = \al d < d \]
    Since a more negative $d'$ can always be found, it must be that $d$ is unbounded. This is a demonstration of the fundamental \textit{Unboundedness Property} of \cite{Bradley_1977}; if the dual is unbounded, then the primal is infeasible.

    \comment[TC]{\href{http://web.stanford.edu/~ashishg/msande111/notes/chapter4.pdf}{Farkas's lemma} here?}

    \begin{definition} An \term{infeasibility certificate} \cite{Andersen_2001} is any vector $y$ that satisfies the constraints of \cref{def:dual_marginal_linear_program} and also permits violation of \cref{eq:dual_marginal_ineq} for some marginal distribution vector $\probvec^{\mscenario}$.
    \[ y \in \R^{\abs{\Out{\mscenario}}} : y \cdot M \succeq 0, \quad y \cdot \probvec^{\mscenario} < 0 \]
    Furthermore, any $y$ satisfying $y \cdot M \succeq 0$ induces a \term{certificate inequality} that constraints the space of marginal distribution vectors which takes the symbolic form of \cref{eq:dual_marginal_ineq},
    \[ y \cdot \probvec^{\mscenario} \geq 0 \]
    Where the entries of the certificate $y$ act as coefficients for the entries of $\probvec^{\mscenario}$.
    \end{definition}
    \todo[TC]{Discuss Infeasibility Certificates basis}
    \todo[TC]{Discuss the certificate inequalities we found.}

    \[ \mathcal{I}\tsb{Mosek-Cert}, \mathcal{I}\tsb{Cert} \]

    \section{Covering Inequalities}

    Section \ref{sec:certificate_inequalities} discusses how one can obtain a valid compatibility inequality that witnesses incompatibility for a particular marginal model $\prob^{\mscenario}$ by writing the marginal problem as a linear program. It was demonstrated that every incompatible distribution is witness-able by some certificate inequality. However, when a particular marginal model $\prob^{\mscenario}$ is not known, a linear optimization offers no utility.
    \todo[TC]{Motivate why certificates are not enough, want many solutions, want logical foundation}
    % Throughout this section, there will be a few omissions of notation to help clar

    In this section, we will first summarize a method that can be used to obtain a complete solution to the \textit{possibilistic} marginal problem and indirectly offer a \textit{partial} solution to the \textit{probabilistic} marginal problem. Doing so corresponds to enumerating all
    \subsection{Logical Covers}
    In this section, we summarize the familiar notion of a logical cover
    % \begin{lemma}
    %     \label{lemma:logical_extenable_set}
    %     Let $m \in \Out{\mscenario}$ be some marginal outcome of the marginal scenario $\mscenario$. If any marginal model $\prob^{\mscenario}$ is to be non-contextual, the following condition must hold.
    %     \[ m \iff \bigvee_{j \in \Ext[\jointvar]{m}} j \]
    %     The idea being that if a joint distribution exists, then the event $m$ represents partial knowledge of the entire system $\jointvar$; whenever $m$ occurs, exactly one of the extendable joint outcomes has to have occurred.
    % \end{lemma}
    Suppose one has a particular marginal outcome $a \in \Out{\mscenario}$ and a marginal model $\prob^{\mscenario}$ that assigns the probability $\prob[][a]$ to the outcome $a$. If the marginal model $\prob^{\mscenario}$ is to admit a joint distribution $\prob[\jointvar]$, then it must be that $\prob[][a]$ is equal to a marginalization over $a$'s extendable set into $\jointvar$.
    \[ \prob[][a] = \sum_{j \in \Ext[\jointvar]{a}}\prob[\jointvar][j] \eq \label{eq:marginal_extendable_sum_a}\]
    This corresponds to row $a$ of the incidence matrix $M$. Next consider a subset of marginal outcomes $C \subseteq \Out{\mscenario}$. Summing over the associated probabilities $\bc{\prob[][c]}_{c \in C}$ and applying \cref{eq:marginal_extendable_sum_a} yields,
    \[ \sum_{c \in C}\prob[][c] = \sum_{c \in C}\sum_{j \in \Ext[\jointvar]{c}}\prob[\jointvar][j] \eq \label{eq:marginal_extendable_sum_cs} \]
    If for all joint distributions $\prob[\jointvar]$ it can be demonstrated that the following holds,
    \[ \sum_{c \in C}\sum_{j \in \Ext[\jointvar]{c}}\prob[\jointvar][j] \geq \sum_{j \in \Ext[\jointvar]{a}}\prob[\jointvar][j] \eq \label{eq:extendable_cover_first} \]
    Then combining \cref{eq:marginal_extendable_sum_a,eq:marginal_extendable_sum_cs,eq:extendable_cover_first} yields a valid compatibility inequality for $\mscenario$.
    \[ \prob[][a] \leq \sum_{c \in C}\prob[][c] \eq \label{eq:1ncover} \]
    In order for \cref{eq:extendable_cover_first} to hold for \textit{all} joint distributions $\prob[\jointvar]$, it must hold on an algebraic level; each term on the right-hand side of \cref{eq:extendable_cover_first} also appears on the left-hand side of \cref{eq:extendable_cover_first}.\footnote{If there were a joint outcome $j^*$ present on the right-hand side of \cref{eq:extendable_cover_first} but not the left, then \cref{eq:extendable_cover_first} would be violated by the deterministic distribution $\prob[\jointvar][j^*] = 1$} Therefore, identifying all compatibility inequalities that can be written in the form of \cref{eq:1ncover} corresponds to identifying all sets $C \subseteq \Out{\mscenario}$ such that their joint distribution terms algebraically \textit{cover} the joint distribution terms for $a \in \Out{\mscenario}$.

    The notion of forming a

    \subsection{Extendable Covers}
    We will now generalize the notion of a logical cover by considering more exotic antecedents corresponding to multiple marginal outcomes $A \subseteq \Out{\mscenario}$ along with weights $\mul{A}$ for each outcome. Afterwards we demonstrate that these generalized covers also admit non-trivial probabilistic compatibility inequalities.

    \begin{definition}
        A \term{multiplicity set} or \term{m-set} $\mul{S}$ for a marginal scenario $\mscenario$ is a function $\mul{S} : \Out{\mscenario} \to \Z_{\geq 0}$ that assigns to each marginal outcome $m \in \Out{\mscenario}$ a \textit{multiplicity} $\mul{S}\br{m} \in \Z_{\geq 0}$. The subscript $S$ is a subset of marginal outcome $S \subseteq \Out{\mscenario}$ indicating which marginal outcomes receive non-zero multiplicity.
        \[ \forall s \in S : \mul{S}\br{s} \in \N  \quad \forall m \in \Out{\mscenario}\setminus S : \mul{S}\br{m} = 0 \]
        We define the \term{extension of an m-set} $\mul{S}$ into $\jointvar$ to be the m-set $\mul{\mext{S}}$ which assigns multiplicities to \textit{joint} outcomes $j \in \Out{\jointvar}$. The extension of $S$ denoted $\mext{S}$ is the set of all joint outcomes that are extensions of marginal outcomes $s \in S$.
        \[ \mext{S} = \bigcup_{s \in S} \Ext[\jointvar]{s} \]
        While the multiplicity of each element $j \in \mext{S}$ corresponds to the sum over multiplicities $\mul{S}\br{s}$ where $j \in \Ext[\jointvar]{s}$.
        \[ \forall j \in \mext{S} : \mul{\mext{S}}\br{j} = \sum_{s \in S} \mul{S}\br{s} \isext{s}{j} \eq \label{eq:joint_multiplicity}\]
        With all other joint outcomes receiving a zero multiplicity $\forall j \not\in \mext{S} : \mul{\mext{S}}\br{j} = 0$.
    \end{definition}
    \begin{definition}
        An m-set $\mul{C}$ is an \term{extendable cover} of an m-set $\mul{A}$ for a marginal scenario $\mscenario$ if and only if the following two conditions hold\footnote{In fact \cref{eq:multi_extenable_cover_def} implies \cref{eq:flat_extenable_cover_def} since $\forall j \not \in \mext{A} : \mul{\mext{A}}\br{j} = 0$.}:
        \begin{gather*}
            E_A \subseteq E_C \eq \label{eq:flat_extenable_cover_def}\\
            \forall j \in E_A : \mul{\mext{A}}\br{j} \leq \mul{\mext{C}}\br{j} \eq \label{eq:multi_extenable_cover_def}
        \end{gather*}
    \end{definition}
    Identifying extendable covers is of great importance because of the following theorem.
    \begin{theorem}
        If the m-set $\mul{C}$ is an extendable cover of the m-set $\mul{A}$ for a marginal scenario $\mscenario$, then the following is a valid compatibility $\mathcal{I}_{\mscenario}$ for the marginal scenario $\mscenario$.
        \[ \sum_{a \in A}\mul{A}\br{a}\prob[][a] \leq \sum_{c \in C}\mul{C}\br{c}\prob[][c] \eq \label{eq:extendable_cover} \]
    \end{theorem}
    \begin{proof}
        To prove that \cref{eq:extendable_cover} is a valid compatibility inequality $\mathcal{I}_{\mscenario}$ for all marginal models $\prob^\mscenario$, we will demonstrate that \cref{eq:extendable_cover} follows from the assumption that a joint distribution $\prob[\jointvar]$ exists.

        For each m-set $\mul{S}$ write the weighted sum over the probabilities $\bc{\prob[][s]}_{s \in S}$ in terms of the joint distribution $\prob[\jointvar]$ using \cref{eq:joint_multiplicity}.
        \begin{align*}
        \eq \label{eq:derivation_cover}
        \begin{split}
        \sum_{s \in S} \mul{S}\br{s}\prob[][s] &= \sum_{s \in S} \mul{S} \br{s} \sum_{j \in \Ext[\jointvar]{s}} \prob[\jointvar][j] \\
        &= \sum_{s \in S} \mul{S} \br{s} \sum_{j \in \Out{\jointvar}} \isext{s}{j} \prob[\jointvar][j] \\
        &=  \sum_{j \in \Out{\jointvar}} \bs{\sum_{s \in S} \mul{S} \br{s} \isext{s}{j}} \prob[\jointvar][j] \\
        &= \sum_{j \in E_{S}} \mul{\mext{S}}\br{j} \prob[\jointvar][j]
        \end{split}
        \end{align*}
        First examine the substitution $S \to C$,
        \[ \sum_{c \in C} \mul{C}\br{c}\prob[][c] = \sum_{j \in E_{C}} \mul{\mext{C}}\br{j} \prob[\jointvar][j] \]
        By \cref{eq:flat_extenable_cover_def} $E_A \subseteq E_C$,
        \[ \sum_{c \in C} \mul{C}\br{c}\prob[][c] = \sum_{j \in E_{A}} \mul{\mext{C}}\br{j} \prob[\jointvar][j] + \sum_{j \in E_{C} \setminus E_{A}} \mul{\mext{C}}\br{j} \prob[\jointvar][j] \]
        Notice that the sum over $j \in E_{C} \setminus E_{A}$ is entirely positive since $\mul{\mext{C}}\br{j} \in \N$ and $\prob[\jointvar][j] \geq 0$. Moreover \cref{eq:multi_extenable_cover_def} gives $\mul{\mext{C}}\br{j} \geq \mul{\mext{A}}\br{j}$. Combining these observations along with \cref{eq:derivation_cover} for $S \to A$, one arrives at the inequality,
        \[ \sum_{c \in C} \mul{C}\br{c}\prob[][c] \geq \sum_{j \in E_{A}} \mul{\mext{A}}\br{j} \prob[\jointvar][j] = \sum_{a \in A} \mul{A}\br{a}\prob[][a] \]
        Which is equivalent to \cref{eq:extendable_cover}; every non-contextual marginal model $\prob^{\mscenario}$ must satisfy \cref{eq:extendable_cover}.
    \end{proof}
    % \begin{corollary}
    %     If the m-set $\mul{C}$ is an extendable cover of the m-set $\br{A, \mulA}$ for a marginal scenario $\mscenario$, then the following is a valid compatibility $\mathcal{I}_{\mscenario}$ for the marginal scenario $\mscenario$.
    %     \[ \sum_{a \in A}\prob[][a] \leq \sum_{c \in C}\prob[][c] \eq \label{eq:extendable_cover_flat} \]
    % \end{corollary}
    % \begin{proof}
    %     This follows from the fact that if $\mul{C}$ is an extendable cover of $\br{A, \mulA}$, then $\br{C, \mathbf{1}_{C}}$ is an extendable cover of $\br{A, \mathbf{1}_{A}}$. Since $E_A \subseteq E_C$ is already trivially satisfied, all that remains is to demonstrate that \cref{eq:multi_extenable_cover_def} holds for unitary multiplicities $\mathbf{1}_A$ and $\mathbf{1}_C$.

    %     First examine extension $\br{E_C, \mul{\mext{C}}}$ for the m-set $\mul{C}$ as defined by \cref{eq:joint_multiplicity},
    %     \[ \forall j \in E_C : \mul{\mext{C}}\br{j} = \sum_{c \in C} \mulC\br{c} \isext{c}{j} \]


    %     First apply \cref{lemma:logical_extenable_set} to an arbitrary set $S \in \bc{A, C}$,
    %     \[ \bigvee_{s \in S} s \iff \bigvee_{s \in S} \bigvee_{j \in \Ext[\jointvar]{s}} j \iff \bigvee_{j \in E_S} j \eq \label{eq:logical_iff_ext}\]
    %     Now since $E_A \subseteq E_C$ by \cref{eq:flat_extenable_cover_def} we have that,
    %     \[ \bigvee_{j \in E_A} j \implies \bigvee_{j \in E_C} j \eq \label{eq:logical_cover}\]
    %     Combining \cref{eq:logical_iff_ext,eq:logical_cover} one arrives at,
    %     \[ \bigvee_{a \in A} a \implies \bigvee_{c \in C}c \]
    % \end{proof}
    % For a working example, let $\jointvar = \bc{a, b, c}$ be a set of binary random variables ($O = \bc{0,1}$) and $\mscenario = \bc{\bc{a,b},\bc{b,c},\bc{a,c}}$ be the marginal context.

    \subsection{Complete Solution to The Marginal Problem}
    \begin{theorem}
        \label{thm:complete_solution_ext_cover}
    \end{theorem}
    \subsection{Extendable Cover Procedure}
    The \term{extendable cover procedure} begins with an antecedent m-set $\mul{A}$ and iteratively produces consequent m-sets $\mul{C}$ that are extendable covers of $\mul{A}$. To facilitate this, we define a \term{partial consequent m-set} to be any marginal m-set $\mul{S}$ that \textit{has yet to} become an extendable cover of $\mul{A}$. Also define the null m-set $\mul{\emptyset}$ to be the m-set assigning zero multiplicity to each marginal outcome.


    \begin{algorithm}[H]
    \caption{The Extendable Cover Procedure (ECP)}
    \label{alg:ecp}
    \begin{algorithmic}
      \State $\mul{S}^{0} = \mul{\emptyset}$
    \end{algorithmic}
    \end{algorithm}

    % \begin{algorithm}
    %   \caption{The Extendable Cover Procedure (ECP)
    %     \label{alg:ecp}}
    %   \begin{algorithmic}[1]
    %     \Require{An antecedent m-set $\mul{A}$ and a partial consequent m-set $\mul{S}$.}
    %     \Statex
    %     \Function{ECP}{$\mul{A}, \mul{S}$}
    %       % \Let{$z$}{$x \oplus y$} \Comment{$\oplus$: bitwise exclusive-or}
    %       % \Let{$\delta$}{$0$}
    %       % \For{$i \gets 1 \textrm{ to } n$}
    %       %   \If{$z_i \neq 0$}
    %       %     \Let{$\delta$}{$\delta + 1$}
    %       %   \EndIf
    %       % \EndFor
    %       \State \Return{$\delta$}
    %     \EndFunction
    %   \end{algorithmic}
    % \end{algorithm}

    \todo[TC]{Trivial nodes}
    \todo[TC]{Duplication under Deflation}
    \todo[TC]{Memory}
    \todo[TC]{Parallelization}
    \todo[TC]{Simplicial Complex}
    \subsubsection{Optimizations}


    \subsubsection{Avoiding Trivial Inequalities}
    A \term{trivial compatibility inequality} $\mathcal{I}_{\mscenario}$ is an inequality that is satisfied by all marginal models $\prob^{\mscenario}$, not just compatible ones. Trivial compatibility inequalities are of no importance Since the set of all extendable cover inequalities
    \todo[TC]{}
    \subsection{}
    \subsection{Irreducible Covers}
    \todo[TC]{Basis certificates}
    \subsection{Limited Factorization}

    \subsection{Logical Implications \& Inequalities}
    \label{sec:implication_inequalities}
    Following the work conducted by Mansfield and Fritz \cite{Mansfield_2012}, we consider a possibilistic implications of the form,
    \[ a \implies c_1 \lor \cdots \lor c_m = \bigvee_{i\in \bs{m}} c_i \eq \label{eq:1n_pi}\]
    Where $a$ and each of the $c_i$'s are simply events or outcomes of a particular set of variables. The letter `$a$' is chosen for the event $a$ since it takes the place of the logical \term{antecedent} of \cref{eq:1n_pi}. Likewise, the letter `$c$' is chosen to represent logical \term{consequents}. We refer to the set of all $c_i$'s simply as $C = \bc{c_i \mid i \in \bs{m}}$. The implication \cref{eq:1n_pi} can be read as \textit{whenever $a$ occurs, at least one element of $c$ also occurs}.

    It is possible to turn possibilistic implications into probabilistic inequalities by recognizing that the logical implication of \cref{eq:1n_pi} induces the inequality,
    \[ \prob[][a] \leq \prob[][\bigvee_{i=1}^{m} c_i] \]
    Furthermore utilizing Boole's inequality,
    \[ \prob[][\bigvee_{i=1}^{m} c_i] \leq \sum_{i=1}^{m} \prob[][c_i] \eq \label{eq:booles} \]
    Gives,
    \[ \prob[][a] \leq \sum_{i=1}^{m} \prob[][c_i] \eq \label{eq:1n_ineq} \]
    Such that whenever the inequality \cref{eq:1n_ineq} is violated, the implication in \cref{eq:1n_pi} is violated as well. Note that the converse is \textit{not} true; if the inequality \cref{eq:1n_ineq} holds true, it is still possible for there to be a violation of \cref{eq:1n_pi}.

    \begin{remark}
        An important result of Boole's inequality is that \cref{eq:booles} becomes an exact \textit{equality} whenever elements of $C$ are pairwise disjoint. Therefore, finding a set $C$ of pairwise disjoint events satisfying \cref{eq:1n_pi} will give rise to tighter inequalities.
    \end{remark}

    \subsection{Implications in the Marginal Problem}
    \label{sec:implications_marginal_problem}
    The question then remains, \textit{how does non-contextuality give rise to implications of the form of \cref{eq:1n_pi}?} Begin by making the principle assumption that a joint distribution \textit{does} exist for a marginal model $\prob^{\mscenario}$. This assumption induces logical \textit{tautologies} that take the form of \cref{eq:1n_pi}, which we call \term{marginal implications}, using the following train of logic.
    \begin{enumerate}
        \item Suppose a particular marginal outcome $a \in O_{\mscenario}$ happens to occur.
        \item Since a joint distribution exists, then $a$ refers to some \textit{incomplete} knowledge about a \textit{joint} event $j = \outc{\jointvar} \in O_{\jointvar}$ that actually occured. More precisely, this set of possible $j$'s is the extendable set of $a$ in $O_{\jointvar}$.
        \[ j \in \Ext[\jointvar]{a} \]
        \item Therefore whenever $a$ occurs, one of the elements $j$ of $\Ext[\jointvar]{a}$ \textit{has} to occur.
        \[ a \implies \bigvee_{j \in \Ext[\jointvar]{a}} j \eq \label{eq:A_impies_extendable}\]
        \item Now suppose we could obtain a set of marginal outcomes $C = \bc{c_1, \ldots, c_m}$ each different from $a$ such that for every $j$, $\Ext{c \to j}$ for at least one element $c \in C$. If such a $C$ can be found, then the possibility of at least one $j$ occurring implies the possibility of at least one $c$ occurring.
        \[ \bigvee_{j \in \Ext[\jointvar]{a}} j \implies c_1 \vee \cdots \vee c_m = \bigvee_{i \in \bs{m}} c_i\eq \label{eq:extendable_implies_cover} \]

        \item Combing \cref{eq:A_impies_extendable} with \cref{eq:extendable_implies_cover}, one obtains \cref{eq:1n_pi}.
    \end{enumerate}

    \todo[TC]{Talk about why this is called hardy paradox}

    Finding all marginal implications for a chosen marginal outcome $a$ corresponds to finding a set of marginal outcomes $C = \bc{c_1, \ldots, c_m}$ whose extendable sets \textit{cover} $\Ext[\jointvar]{a}$. Formally this corresponds to a \term{set covering} problem \todo[TC]{cite} which we elected to cast as the equivalent \term{hypergraph transversal problem}.

    \todo[TC]{Mention sufficient solution to the possibilistic marginal problem}
    \todo[TC]{Illustrate how it can distinguish more than possibilistic differences}

    Given an antecedent $a$, we can construct a hypergraph $\hgraph_{a}$ called the \term{marginal hypergraph} with nodes $\nodes_a$ and $\edges_a$. The nodes of this hypergraph are the subset of marginal outcomes $\nodes_a \subseteq O^{\mscenario}$ \textit{compatible} with $a$.
    The edges are labeled by outcomes in $\Ext[\jointvar]{a} \subseteq O_{\jointvar}$, namely the set of $j$'s, and contain all nodes compatible with $j$.
    \begin{align*}
        \nodes_a &= \bc{n \in O^{\mscenario} \mid \Com{n, a}} \eq \label{eq:hgraph_nodes} \\
        \edges_a &= \bc{\bc{n \in O^{\mscenario} \mid \Com{n, j}} , j \in \Ext[\jointvar]{a}} \eq \label{eq:hgraph_edges}
    \end{align*}
    \begin{remark}
        \label{remark:trivial_transversal}
        Note that every node is contained in some edge. To prove this, consider the definition of compatibility between outcomes. If every $n$ is compatible with $a$, then there exists some outcome $j \in \Ext[\jointvar]{A}$ such that $\Ext{n \to j}$. Therefore since $\Ext{n \to j}$, $\Com{n, j}$; satisfying the central condition of \cref{eq:hgraph_edges}.
    \end{remark}

    \subsection{Higher Order Marginal Implications}

    \todo[TC]{Discuss $(m,n)-type$ implications and the non-triviality}
    \todo[TC]{Link to logical bell inequalities/completeness or not?}

    \subsection{Hypergraph Transversals}
    Section \ref{sec:implications_marginal_problem} demonstrates how finding possibilistic marginal implications can be casted as a hypergraph transversal problem. This section aims to summarize the general idea behind existing algorithms and also discusses some of the inequalities found using these algorithms. \todo[TC]{References here }

    \begin{definition}
        A \term{hypergraph transversal generation} is any algorithm that correctly generates the complete set of all minimal transversals of $\mathcal{H}$.
    \end{definition}

    \begin{definition}
        Any hypergraph with strictly non-empty edges $\forall e \neq \emptyset$ will always admit the \term{trivial transversal} $\trans^*$ where all nodes are considered as members $\trans^* = \nodes$. Any hypergraph with empty edges will be called a \term{degenerate hypergraph} as it admits no transversals\footnote{Most authors require that all hypergraphs be non-degenerate \cite{Kavvadias_2005}.}.
    \end{definition}
    Remark \ref{remark:trivial_transversal} guarantees that all marginal hypergraphs are non-degenerate. There are two distinct approaches to hypergraph transversal generation: \textit{top-down} and \textit{bottom-up}.
    \begin{definition}
        A \term{top-down} hypergraph transversal generation refers to any algorithm that begins with the trivial transversal $\trans^*$ and iteratively removes unnecessary nodes from $\trans^*$.
    \end{definition}

    \begin{definition}
        A \term{bottom-up} hypergraph transversal generation refers to any algorithm that begins with the empty set $\emptyset$ and iteratively adds nodes.
    \end{definition}

    One should select a top-down method if the typical size of minimal transversals $\abs{\trans}$ is comparable to the number of nodes $\abs{\nodes}$, otherwise a bottom-up method will perform better.
    For our purposes we implemented a deep-first transversal algorithm similar to
    Recalling \cref{remark:trivial_transversal}

    \subsection{Weighted Hypergraph Transversals}
    In \cref{sec:certificate_inequalities} is was demonstrated that the Fritz distribution is witness-able via a certificate inequality. It is also possible to witness
    \todo[TC]{Discuss the Inequalities Derived/ Trivial and non-trivial}

    \todo[TC]{Weighted transversals and Optimizations}
    \todo[TC]{Seeding inequalities (huge advantage here)}
    \section{Deriving Symmetric Inequalities}
    \label{sec:symmetry}
    Symmetric compatibility inequalities are useful for a number of reasons. First, Bancal et. al. \cite{Bancal_2010} discuss computational advantages in considering symmetric versions of marginal polytopes mentioned in \cref{sec:comp_con_mp}; the number of extremal points typically grows exponentially in $\jointvar$, but only polynomial for the symmetric polytope. They also note a number of interesting inequalities (such as CHSH \cite{CHSH_Original}) can be written in a way that is symmetric under the exchange of parties, demonstrating that quantum-non-trivial inequalities can be recovered from facets of a symmetric polytope. Second, numerical optimizations against inequalities invariant under exchange of parties will lead to one of two interesting cases: either the extremal distribution is invariant under exchange of parties or it is not. The latter case generates a family of incompatible distributions obtained by exchanging parties of the found extremal distribution\footnote{If the extremal distribution is not invariant under exchange of parties, there is indication that the space of accessible distributions is non-convex.}. In this section we discuss how to achieve computational advantage and its application to the inequality techniques mentioned in \cref{sec:certificate_inequalities} and \cref{sec:implication_inequalities}.
    \subsection{Causal Symmetry}
    The aim of this section is to formalize what types of symmetries are present in a particular causal structure $\graph$. The group of these causal symmetries will define, for each compatibility inequality $\mathcal{I}_{\mscenario}$, a family of inequalities that are also valid incompatibility inequalities.

    First consider the permutation group acting on a set of nodes $N \subseteq \nodes$ denoted $\Perm{N}$ to be the set of all bijective maps $\gelem$ from $N$ to $N$.
    \[ \Perm{N} = \bc{\gelem \mid \gelem : N \to N} \]
    The action of $\gelem \in \Perm{N}$ on a causal structure $\graph$ is defined via an the extension of $\gelem$ to the corresponding element in $\Perm{\nodes}$ that leaves nodes $n \in \nodes \setminus N$ invariant,
    \begin{align*}
    \action{\graph} &\defined \br{\action{\nodes}, \action{\edges}} \\
    \action{\nodes} &\defined \bc{\action{n} \mid n \in \nodes} = \nodes \\
    \action{\edges} &\defined \bc{\action{e} \mid e \in \edges}
    \end{align*}
    To motivate a more general treatment of such symmetries, consider the Triangle Scenario of \cref{fig:triangle_scenario}. Due to the rotational and reflective symmetries of the Triangle Scenario, any permutation $\gelem \in \Perm{\nodes_O}$ of the observable nodes $\nodes_O = \bc{A, B, C}$ creates a new causal structure $\action{\graph}$ that is \textit{equivalent} to $\graph$ up to a relabeling of \textit{latent} nodes. By construction, any valid compatibility inequality $\mathcal{I}_{\mscenario}$ for the marginal scenario $\mscenario = \bc{V_1, \ldots, V_k}$ is \textit{independent} of the labeling of latent nodes. Therefore applying $\gelem$ to all distributions in $\mathcal{I}_{\mscenario}$ yields $\action{\mathcal{I}_{\mscenario}} \defined \action{\mathcal{I}}_{\action{\mscenario}}$, another valid compatibility inequality for the permuted marginal scenario $\action{\mscenario}$ defined as,
    \[ \action{\mscenario} = \bc{\action{V_1}, \ldots, \action{V_k}} \]
    % \begin{example}
    %     \todo[TC]{Above trivial example.}
    %     Consider the marginal scenario $V_{\bs{4}} = \bc{\bc{A,B,C}, \bc{A}, \bc{B}, \bc{C}}$ and the compatibility inequality $\mathcal{I}_{\bc{ABC, A, B, C}}$ for the Triangle Scenario in \cref{fig:triangle_scenario} with the following form,
    %     \[ \prob[ABC][002]\prob[ABC][003] \leq \prob[A][0]\prob[C][3] \]
    %     Now consider $\gelem = \bc{A \to A, B \to C, C \to B}$. Since $\action{\graph} = \graph$, $\action{\mathcal{I}_{\mscenario}}$ is also a valid compatibility inequality,
    % \end{example}
    More generically, any permutation $\gelem$ that preserves the graphical structure of $\graph$ can be applied to $\mathcal{I}_{\mscenario}$ to generate new and valid compatibility inequalities. Specifically this alludes to the \term{graph automorphism group} $\Aut{\graph}$ of $\graph$.
    \[ \Aut{\graph} \defined \bc{\gelem \in \Perm{\nodes} \mid \action{\graph} = \graph} \]
    In general, $\Aut{\graph}$ could include elements $\gelem$ that map latent nodes to observable nodes\footnote{In practice however, it is rare to be considering a causal structure $\graph$ where some latent node $n_L \in \nodes_L$ takes on an role indistinguishable from some observable node $n_O \in \nodes_O$, so this won't be an issue.}. This behaviour is \textit{undesired} for causal inference where the latent nodes will never appear in a marginal scenario. Instead, the subgroup of $\Aut{\graph}$ that never maps observable nodes to latent nodes is the \term{set-wise stabilizer} of $\Aut{\graph}$ for $\nodes_O$ and will be expressed with a subscript.

    \begin{definition}
        The \term{causal symmetry group} for a causal structure $\graph$ is the graph automorphism subgroup that stabilizes the observable nodes.
        \[ \Aut[\nodes_O]{\graph} = \bc{\gelem \in \Perm{\nodes} \mid \action{\graph} = \graph, \forall n \in \nodes_O :\action{n} \in \nodes_O} \]
        Given any compatibility inequality $\mathcal{I}_{\mscenario}$, each element $\gelem$ of the causal symmetry group defines a new valid compatibility inequality $\action{\mathcal{I}_{\mscenario}} = \action{\mathcal{I}}_{\action{\mscenario}}$.
    \end{definition}

    The inflation technique discussed in \cref{sec:summary_inflation} allows one to derive compatibility inequalities for a causal structure $\graph$ by considering the marginal problem over the pre-injectable sets of $\graph'$ denoted $\PreInj[\graph]{\graph'} = \bc{V_1, \ldots, V_k}$. It is important to recall that due to \cref{lem:inflation}, inequalities $\mathcal{I}'$ for $\graph'$ are \textit{only} transferable to inequalities $\mathcal{I}$ for $\graph$ if $\mathcal{I}'$ is in terms of distributions over the pre-injectable sets $\mathcal{I}'_{\PreInj[\graph]{\graph'}}$. As a consequence of this observation, we need to consider a subgroup of the causal symmetry group that preserves the pre-injectable sets,
    \[ \action{\PreInj[\graph]{\graph'}} = \PreInj[\graph]{\graph'} \eq \label{eq:preserved_preinj} \]

    \begin{definition}
        The \term{restricted causal symmetry group} $\Phi$ for a causal structure $\graph$ and a marginal scenario $\mscenario$ is the graph automorphism subgroup that stabilizes the marginal scenario $\mscenario$.
        \[ \Phi \defined \Aut[\mscenario]{\graph} = \bc{\gelem \in \Perm{\nodes} \mid \action{\graph} = \graph, \forall V \in \mscenario : \action{V} \in \mscenario} \]
        Given any compatibility inequality $\mathcal{I}_{\mscenario}$ each element $\gelem$ of the causal symmetry group defines a new valid compatibility inequality $\action{\mathcal{I}_{\mscenario}} = \action{\mathcal{I}}_{\mscenario}$ over the \textit{same} marginal context $\mscenario$.
    \end{definition}

    The restricted causal symmetry group for the pre-injectable sets of an inflation $\graph'$ is the ideal symmetry group for use with the inflation technique. If one obtains a particular inequality $\mathcal{I}'_{\PreInj[\graph]{\graph'}}$ as well as the restricted causal symmetry group $\Phi$, an entire family of valid compatibility inequalities are also obtained.
    \[ \bc{\action{\mathcal{I}'}_{\PreInj[\graph]{\graph'}} \mid \gelem \in \Phi} \text{ where } \Phi = \Aut[\PreInj[\graph]{\graph'}]{\graph'} \]
    We have obtained $\Phi$ for the inflated triangle scenario \cref{fig:inflated_triangle_scenario}. $\Phi$ is an order $48$ group with the following $4$ generators:
    \begin{equation*}
    \begin{aligned}[c]
    &\hspace{1mm}\gelem_1 \\
    A_1 &\to A_4 \\A_2 &\to A_3 \\A_3 &\to A_2 \\A_4 &\to A_1 \\B_1 &\to B_4 \\B_2 &\to B_3 \\B_3 &\to B_2 \\B_4 &\to B_1 \\C_1 &\to C_4 \\C_2 &\to C_3 \\C_3 &\to C_2 \\C_4 &\to C_1
    \end{aligned}
    \quad
    \begin{aligned}[c]
    &\hspace{1mm}\gelem_2 \\
    A_1 &\to A_1 \\A_2 &\to A_3 \\A_3 &\to A_2 \\A_4 &\to A_4 \\B_1 &\to C_1 \\B_2 &\to C_3 \\B_3 &\to C_2 \\B_4 &\to C_4 \\C_1 &\to B_1 \\C_2 &\to B_3 \\C_3 &\to B_2 \\C_4 &\to B_4
    \end{aligned}
    \quad
    \begin{aligned}[c]
    &\hspace{1mm}\gelem_3 \\
    A_1 &\to C_1 \\A_2 &\to C_2 \\A_3 &\to C_3 \\A_4 &\to C_4 \\B_1 &\to A_1 \\B_2 &\to A_2 \\B_3 &\to A_3 \\B_4 &\to A_4 \\C_1 &\to B_1 \\C_2 &\to B_2 \\C_3 &\to B_3 \\C_4 &\to B_4
    \end{aligned}
    \quad
    \begin{aligned}[c]
    &\hspace{1mm}\gelem_4 \\
    A_1 &\to A_1 \\A_2 &\to A_2 \\A_3 &\to A_3 \\A_4 &\to A_4 \\B_1 &\to B_2 \\B_2 &\to B_1 \\B_3 &\to B_4 \\B_4 &\to B_3 \\C_1 &\to C_3 \\C_2 &\to C_4 \\C_3 &\to C_1 \\C_4 &\to C_2
    \end{aligned}
    \end{equation*}
    % ((3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8),
    %  (0, 2, 1, 3, 8, 10, 9, 11, 4, 6, 5, 7),
    %  (8, 9, 10, 11, 0, 1, 2, 3, 4, 5, 6, 7),
    %  (0, 1, 2, 3, 5, 4, 7, 6, 10, 11, 8, 9))
    It is easy to verify that $\gelem_1, \gelem_2, \gelem_3, \gelem_4$ are all automorphisms of the inflated Triangle Scenario $\graph'$ of \cref{fig:inflated_triangle_scenario}. Moreover they stabilize the observable nodes, and the map pre-injectable sets of \cref{eq:pre-injectable_triangle_scenario} to the pre-injectable sets. Finally, in the Triangle Scenario we have that,
    \[ \Phi = \Aut[\PreInj[\graph]{\graph'}]{\graph'} \sim \Aut[\nodes_O]{\graph} \]
    To see this, $\gelem_1$ and $\gelem_4$ become the identity element $\mathbb{I}$ in $\Aut[\nodes_O]{\graph}$ upon removal of the copy-index, leaving $\gelem_2$ to generate reflections and $\gelem_3$ to generate rotations.
    \subsection{Symmetric Marginal Polytope}
    Up until this point, we have discussed how new inequalities can be created from known ones by exploiting causal symmetry. Although very useful, we divert our attention to finding \textit{symmetric} inequalities themselves.
    \[ \forall \gelem \in \Phi :  \action{\mathcal{I}_{\mscenario}} = \mathcal{I}_{\mscenario} \]
    In order to do this, we must first define how elements $\gelem \in \Phi$ act on marginal outcomes $O_{\mscenario}$.
    \begin{definition}
        Let $\jointvar = \bc{v_1, \ldots, v_k}$ be a collection of random variables and let $\gelem \in \Perm{\jointvar}$. $\jointvar$ is said to be \term{label invariant with respect to $\gelem$} if the outcome labels for each variable $v \in \jointvar$ are unchanged under the action of $\gelem$.
        \[ \forall v \in \jointvar : O_{v} = O_{\action{v}} \]
        Label invariance is a prerequisite for any kind of symmetry amoung random variables.
    \end{definition}
    \begin{definition}
        Let $\jointvar = \br{v_1, \ldots v_k}$ be a collection of random variables along with a specific outcome $m \in \Out{V}$ where $V \subseteq \jointvar$. The action of $\gelem \in \Perm{\jointvar}$ on $m$ is a new outcome $\action{m} \in \Out{\action{V}}$ over $\action{V} \subseteq \jointvar$ defined as follows,
        \[ \forall v \in V : \br{\action{m}}\br{\action{v}} \defined m \br{v} \]
        The idea being that $\gelem$ modifies $m$ in such a way that permutes its domain while leaving its range invariant.
    \end{definition}
    \begin{example}
        Consider the outcome $m = \bc{A \mapsto 0, B \mapsto 1, C \mapsto 2} \in \Out{\bc{A,B,C}}$ where all variables have $4$ possible outcomes $O_A = O_B = O_C = \bc{0,1,2,3}$ and the permutation $\gelem = \bc{A \to D, B \to C, C \to B, D \to A}$.
        The \textit{action} of $\gelem$ on $m$ is as follows,
        \begin{align*}
            \action{m} &= \action{\bc{A \mapsto m\br{A}, B \mapsto m\br{B}, C \mapsto m\br{C}}} \\
            &= \bc{\action{A} \mapsto m\br{A}, \action{B} \mapsto m\br{B}, \action{C} \mapsto m\br{C}} \\
            &= \bc{D \mapsto m\br{A}, C \mapsto m\br{B}, B \mapsto m\br{C}} \\
            &= \bc{D \mapsto 0, C \mapsto 1, B \mapsto 2} \\
            &= \bc{B \mapsto 2, C \mapsto 1, D \mapsto 0}
        \end{align*}
        Evidently, $\action{m} \in \Out{\bc{B, C, D}}$.
    \end{example}

    Through repeated action of $\gelem \in \Phi$ on marginal outcomes $m \in \Out{\mscenario}$ and joint outcomes $j \in \Out{\jointvar}$, one can define group orbits of $\Phi$ in $\Out{\mscenario}$ and $\Out{\jointvar}$.
    \begin{align*}
        \Orb[\Phi]{m} &\defined \bc{\action{m} \mid \gelem \in \Phi} \\
        \Orb[\Phi]{j} &\defined \bc{\action{j} \mid \gelem \in \Phi}
    \end{align*}

    \begin{definition}
        The \term{symmetric incidence matrix} $M_{\Phi}$ for a marginal scenario $\mscenario$ and a particular restricted causal symmetry $\Phi$ is a contracted version of the incidence matrix $M$ for $\mscenario$. Each row of $M_{\Phi}$ corresponds to a marginal orbit $\Orb[\Phi]{m}$. Analogously each column of $M_{\Phi}$ corresponds to a joint orbit $\Orb[\Phi]{j}$. The entries of $M_{\Phi}$ are integers and correspond to summing over the rows and columns of $M$ that belong to each orbit.
        \[ M_{\Phi}\bs{\Orb[\Phi]{m}, \Orb[\Phi]{j}} = \sum_{\substack{j' \in \Orb[\Phi]{j} \\ m' \in \Orb[\Phi]{m}}} M\bs{m',j'} \]
    \end{definition}


    \subsection{Symmetric Logical Implications}
    \subsection{Results}

    \todo[TC]{Identify the desired symmetry group}
    \todo[TC]{How we obtained the desired symmetry group}
    \todo[TC]{Group orbits to symmetric marginal description matrix}
    \todo[TC]{Infeasibility on symmetric marginal problem}
    \todo[TC]{Hardy Transversals can't work on the symmetric marginal problem}
    \todo[TC]{Symmetrizing non-symmetric inequalities through avoiding orbits}
    \todo[TC]{higher order transversals on mutually impossible events}
    \section{Non-linear Optimizations}
    \label{sec:optimizations}
    \todo[TC]{Mention outlook on te importance of classification and determining interesting inequalities \cite{Bancal_2010}}
    Compatibility inequalities for a given causal structure are fantastic for finding incompatible distributions. In the inflation technique, this is no exception. Parameterizing a space distributions using a set of real-valued parameters $\lambda$, enables us to perform numerical optimizations against these inequalities in hopes that a particular set of parameters $\lambda$ is able to generate an incompatible distribution $\prob$. To illustrate this generic procedure and it's reliability, we will first examine the popular CHSH inequality.
    \subsection{Numerical Violations of The CHSH Inequality}
    The CHSH inequality \cite{CHSH_Original} can we viewed as a causal compatibility inequality for the iconic Bell Scenario (Fig. 19 of \cite{Wood_2012}, Fig. 11 of \cite{Inflation}, Fig. 1 a) of \cite{Tavakoli_2015}, etc.) corresponding to Bell's notion of local causality \cite{Wood_2012}. It constrains the set of 2-outcome bipartite distributions over local binary measurement settings for each party $\prob[AB\mid S_A S_B] \defined \bc{\prob[AB\mid 0 0], \prob[AB\mid 01], \prob[AB\mid 10], \prob[AB\mid 11]}$. Numerical optimization \textit{should} obtain the algebraic violation associated with the PR-Box correlations \cite{PR_1995}. Maintaining full generality, we simply need to parameterize these $4$ distributions using \cref{eq:uniform_param}, each requiring $4$ real-valued parameters. We define the optimization target for the CHSH inequality to be the left-hand-side of \cref{eq:CHSH},
    \[ \mathcal{I}\tsb{CHSH} = \ba{AB|11} + \ba{AB|12} + \ba{AB|21} - \ba{AB|22} \]
    Figure \ref{fig:CHSH_convex} demonstrates this optimization for $5$ random seed parameters $\lambda^0$, each converging to the expected value of $4$. Analogously, \cite{Cirelson_1980}

    \begin{figure}
    \newlength\figureheight
    \newlength\figurewidth
    \setlength\figureheight{2.7in}
    \setlength\figurewidth{.48\textwidth}
    \begin{center}
        \begin{minipage}[b]{.48\textwidth}
            \centering
            \includegraphics{../figures/CHSH_convex.pdf}
            \caption{Convex optimizations against $\mathcal{I}\tsb{CHSH}$ recover algebraic violation of $4$.}
            \label{fig:CHSH_convex}
        \end{minipage}\hspace{0.04\textwidth}%
        \begin{minipage}[b]{.48\textwidth}
            \centering
            \includegraphics{../figures/CHSH_quantum.pdf}
            \caption{Quantum optimizations against $\mathcal{I}\tsb{CHSH}$ recover maximum violation of $2\sqrt{2}$.}
            \label{fig:CHSH_quantum}
        \end{minipage}
    \end{center}
    \end{figure}

    \todo[TC]{Demonstrate Quantum, Convexity}
    \todo[TC]{Why Inequalities are great for optimizations}
    \todo[TC]{Non-linearity}
    \todo[TC]{Techniques Used}
    \todo[TC]{Finding maximum violation of CHSH easily}
    \todo[TC]{Unreliance when number of parameters increases}
    \todo[TC]{Issues with local minimum}
    \todo[TC]{Using initial conditions close to fritz, obtain greater violation}
    \todo[TC]{Greater violation shares possibilistic structure of fritz and violates CHSH under definition}
    \todo[TC]{Not realizable with maximally entangled qubit states}
    \todo[TC]{Not realizable with separable measurements}
    \todo[TC]{Many non-trivial inequalities to be tested}
    \todo[TC]{inequality -> dist -> inequality evolution}
    \section{Conclusions}
    \todo[TC]{Inflation technique allows one to witness fritz incompatibility}
    \todo[TC]{Linear optimization induces certificates which are incompatibility witnesses}
    \todo[TC]{There are quantum distributions in the Triangle Scenario that are incompatible and different from fritz in terms of entanglement but not possibilistic structure}
    \section{Open Questions \& Future Work}
    The marginal problem has be well studied in various contexts \cite[\ldots]{Vorobev_1962,Inflation,Fritz_2011}. Reference \cite{Fritz_2011} contains an excellent summary of its connection to other fields of research including knowledge integration, database theory and coalition games in game theory.
    \todo[TC]{Lots of stuff}
    \appendix
    \section{Exemplary Inequalities}
    \section{Connections to Sheaf-Theoretic Treatment}
    \section{Computationally Efficient Parametrization of the Unitary Group}
    Spengler, Huber and Hiesmayr \cite{Spengler_2010_Unitary} suggest the parameterization of the unitary group $\mathcal{U}\br{d}$ using a $d\times d$-matrix of real-valued parameters $\lambda_{n, m}$,
    \[ U = \bs{\prod_{m=1}^{d-1} \br{\prod_{n=m+1}^{d} \exp\br{i P_n \lambda_{n,m}}\exp\br{i \si_{m,n} \lambda_{m,n}}}} \cdot \bs{\prod_{l=1}^{d} \exp\br{iP_l \lambda_{l,l}}}  \eq \label{eq:spengler_unitary} \]
    Where $P_l$ are one-dimensional projective operators,
    \[ P_l = \ket{l}\bra{l} \eq \label{eq:projective_operator} \]
    and the $\si_{m,n}$ are generalized anti-symmetric $\si$-matrices,
    \[ \sigma_{m,n} = -i \ket{m}\bra{n} +i \ket{n}\bra{m} \]
    Where $1 \leq m < n \leq d$. Spengler et. al. proved the validity of \cref{eq:spengler_unitary} in Ref. \cite{Spengler_2010_Unitary}.

    For the sake of reference, let us label the matrix exponential terms in \cref{eq:spengler_unitary} in a manner that corresponds to their affect on an orthonormal basis $\bc{\ket{1}, \ldots, \ket{d}}$.
    \begin{align}
    \begin{split}
        GP_l &= \exp\br{iP_l \lambda_{l,l}} \\
        RP_{n,m} &= \exp\br{i P_n \lambda_{n,m}} \\
        R_{m,n} &= \exp\br{i \si_{m,n} \lambda_{m,n}}
    \end{split} \eq \label{eq:exp_terms}
    \end{align}
    It is possible to remove the reliance on matrix exponential operations in \cref{eq:spengler_unitary} by utilizing the explicit form of the exponential terms in \cref{eq:exp_terms}. As a first step, recognize the defining property of the projective operators \cref{eq:projective_operator},
    \[ P_l^k = \br{\ket{l}\bra{l}}^k = \ket{l}\bra{l} = P_l \]
    This greatly simplifies the global phase terms $GP_l$,
    \[ GP_l = \exp\br{iP_l \lambda_{l,l}} = \sum_{k=0}^{\inf} \f{\br{iP_l \lambda_{l,l}}^k}{k!} = \mathbb{I} + \sum_{k=1}^{\inf} \f{\br{i \lambda_{l,l}}^k}{k!}P_l^k = \mathbb{I} + P_l \bs{\sum_{k=1}^{\inf} \f{\br{i \lambda_{l,l}}^k}{k!}} = \mathbb{I} + P_l \br{e^{i \lambda_{l,l}} - 1} \eq \label{eq:unitary_GP} \]
    Analogously for the relative phase terms $RP_{n,m}$,
    \[ RP_{n,m} = \cdots = \mathbb{I} + P_n \br{e^{i \lambda_{n,m}} - 1} \eq \label{eq:unitary_RP} \]
    Finally, the rotation terms $R_{m,n}$ can also be simplified by examining powers of $i \sigma_{n,m}$,
    \[ R_{m,n} = \exp\br{i \si_{m,n} \lambda_{m,n}} = \sum_{k=0}^{\inf} \f{\br{\ket{m}\bra{n} - \ket{n}\bra{m}}^k \lambda_{m,n}^k}{k!} \]
    One can verify that the following properties hold,
    \begin{align*}
        \br{\ket{m}\bra{n} - \ket{n}\bra{m}}^0 &= \mathbb{I} \\
        \forall k \in \N, k \neq 0 : \br{\ket{m}\bra{n} - \ket{n}\bra{m}}^{2k} &= \br{-1}^k\br{\ket{m}\bra{m} + \ket{n}\bra{n}} \\
        \forall k \in \N : \br{\ket{m}\bra{n} - \ket{n}\bra{m}}^{2k+1} &= \br{-1}^k\br{\ket{m}\bra{n} - \ket{n}\bra{m}}
    \end{align*}
    Revealing the simplified form of $R_{m,n}$,
    \[ R_{m,n} = \mathbb{I} + \br{\ket{m}\bra{m} + \ket{n}\bra{n}} \sum_{j=1}^{\inf} \br{-1}^j\f{\lambda_{n,m}^{2j}}{\br{2j}!} + \br{\ket{m}\bra{n} - \ket{n}\bra{m}} \sum_{j=0}^{\inf} \br{-1}^j\f{\lambda_{n,m}^{2j+1}}{\br{2j+1}!} \]
    \[ R_{m,n} = \mathbb{I} + \br{\ket{m}\bra{m} + \ket{n}\bra{n}} \br{\cos\lambda_{n,m} - 1} + \br{\ket{m}\bra{n} - \ket{n}\bra{m}} \sin\lambda_{n,m} \eq \label{eq:unitary_R} \]
    By combining the optimizations of \cref{eq:unitary_RP,eq:unitary_R,eq:unitary_GP} together we arrive at an equivalent form for \cref{eq:spengler_unitary} that is computational more efficient.
    \[ U = \bs{\prod_{m=1}^{d-1} \br{\prod_{n=m+1}^{d} RP_{n,m}R_{m,n}}} \cdot \bs{\prod_{l=1}^{d} GP_l} \eq \label{eq:fast_spengler_unitary} \]
    In quantum mechanics, the global phase of a state $\ket{\psi} \in \Hilb^n$ is a \textit{redundant} parameter. Parameterizing unitaries using \cref{eq:fast_spengler_unitary} is especially attractive since the global phase terms $GP_l$ can be dropped, allowing one to parameterize all unitaries in $\mathcal{U}\br{d}$ up to this degeneracy \cite{Spengler_2010_Unitary}\footnote{In our implementation, we accomplish this by explicitly setting $\lambda_{l,l} = 0$ in \cref{eq:unitary_GP}}.
    \[ U_{/ GP_l} = \bs{\prod_{m=1}^{d-1} \br{\prod_{n=m+1}^{d} RP_{n,m}R_{m,n}}} \eq \label{eq:fast_spengler_unitary_gp} \]
    \todo[TC]{Explanation of Computational Complexity $\mathcal{O}\br{d^3}$ vs. $\mathcal{O}\br{1}$ using \cite{Moler_2003}}
    \todo[TC]{Pre-Caching for Fixed dimension $d$}
    \todo[TC]{Talk about inverse via haar measure}

    \section{Parametrization of Quantum States \& Measurements}
    \label{sec:param_quantum_states}
    Throughout \cref{sec:optimizations}, we utilize a variety of parameterizations of quantum states and measurements in order to generate quantum-accessible probability distributions. There are numerous techniques that can used when parameterizing quantum states and measurements \cite{Petz_2015, Hedemann_2013,Spengler_2010_Unitary,Fujii_2005,James_2001} with applications \todo[TC]{Finish this sentence}. For our purposes, we need to parameterize the space of quantum-accessible distributions $\prob[\mathcal{Q}]$ that are \textit{realized} on the Triangle Scenario. We have implemented $\prob[\mathcal{Q}]$ under the following description.
    \[ \prob[ABC]\br{abc} = \Tr\bs{\Omega^\intercal \rho_{AB}\otimes\rho_{BC}\otimes\rho_{CA} \Omega M_{A,a}\otimes M_{B,b} \otimes M_{C,c}} \eq \label{eq:triangle_quantum_distributions} \]

    \subsection{Quantum States}
    The bipartite states $\br{\rho_{AB}, \rho_{BC}, \rho_{CA}}$ of \cref{eq:triangle_quantum_distributions} were taken to be two-qubit density matrices acting on $\Hilb^2 \otimes \Hilb^2$.\footnote{We also considered qutrit $\Hilb^3$ qutit $\Hilb^4$ states. However for $6$ $d$-dimensional $\Hilb^d$ states, the joint density matrix $\rho$ acts on $\br{\Hilb^d}^{\otimes 6}$ making it a $\br{d^6, d^6}$ matrix with $d^{12}$ entries. Computationally only $d = 2$ was feasible for our optimization tasks.} The space of all such states corresponds to the space of all $4\times 4$ positive semi-definite hermitian matrices with unitary trace. Throughout this section, we refer to these bipartite states simply as $\rho$ unless otherwise indicated. There are three distinct techniques that we have considered.

    Taking inspiration from \cite{James_2001}, we can parameterize all such density matrices $\rho$ using \term{Cholesky Parametrization} \cite{Grasmair_2014}. The Cholesky decomposition allows one to write any hermitian positive semi-definite matrix $\rho$ in terms of a lower (or upper) triangular matrix $T$ using $\rho = T^\dagger T$. Our Cholesky parameterization consists of assigning $16$ real-valued parameters $\lambda$ to the entires of $T$ and generating a unitary trace $\rho$ similar to eq. (4.4) of \cite{James_2001}.
    \[ \rho = \f{T^\dagger T}{\Tr\br{T^\dagger T}} \quad T = \pmtrx{\la_{1}&0&0&0\\\la_{2} + i \la_{3}&\la_{4}&0&0\\\la_{5} + i \la_{6}&\la_{7} + i \la_{8}&\la_{9}&0\\\la_{10} + i \la_{11}&\la_{12} + i \la_{13}&\la_{14} + i \la_{15}&\la_{16}} \eq \label{eq:cholesky_param} \]
    Our deviation from exclusiving using \cref{eq:cholesky_param} is two-fold. First, \cref{eq:cholesky_param} is degenerate in that the normalization indicates only $16 - 1 = 15$ parameters are required for fully generic parameterization of all such states $\rho$. Removing this degeneracy is possible although difficult. Second, the parameters $\lambda_i$ carry no physical meaning associated with the state $\rho$, unlike our next parameterization.

    In Spengler, Huber and Hiesmayr's work \cite{Spengler_2010_Unitary}, they discuss how to parameterization density matrices $\rho$ acting on $\Hilb^d$ of rank $k$ through it's spectral decomposition,
    \[ \rho = \sum_{i=1}^{k} p_i \ket{\psi_i}\bra{\psi_i} \quad p_i \geq 0, \sum_{i} p_i = 1, k \leq d \eq \label{eq:unitary_param_density} \]
    Where any orthonormal basis $\bc{\ket{\psi_i}}$ of $\Hilb^d$ can be transformed into a computational basis $\bc{\ket{i}}$ by a unitary $U \in \mathcal{U}\br{d}$ such that $\ket{\psi_i} = U\ket{i}$. We refer to \cref{eq:unitary_param_density} as the \term{Spengler Parametrization}. Without loss of generality we parameterize all full-rank ($k=d$) matrices by simultaneously parameterizing the $d=4$ eigenvalues $p_i$ of \cref{eq:unitary_param_density} using \cref{eq:convex_param} and the unitary group $\mathcal{U}\br{4}$ up to global phase equivalence using \cref{eq:fast_spengler_unitary_gp}. Parameterizing $\rho$ using the Spengler parameterization requires $3 + 12 = 15$ parameters; admitting no degeneracies.

    Finally in cases where we wish to restrict ourselves to \textit{pure} bipartite states $\rho = \ket{\psi}\bra{\psi}$, we have the luxury to use a \term{Schmidt Parametrization}. This is accomplished via a Schmidt decomposition $\ket{\psi_{AB}} = \sum_{i}\sigma_i \ket{i_A}\otimes\ket{i_B}$ where normalization demands that $\sum_{i} \sigma_i^2 = 1$, $\bc{\ket{i_A}}$ and $\bc{\ket{i_B}}$ are orthonormal bases for $\Hilb_A$ and $\Hilb_B$ respectively \cite{Neilsen_Chaung_2011}. Additionally for qubit sources we can \textit{choose} our orthonormal bases to be the computational basis $\bc{\ket{0}, \ket{1}}$ and write,
    \[ \ket{\psi} = \cos^2\br{\lambda_1} \ket{0} \otimes \ket{0} + \sin^2\br{\lambda_1} \ket{1} \otimes \ket{1} \]
    Where only $1$ real-valued parameter $\lambda = \bc{\lambda_1}$ is required to parameterize all pure states up to local unitaries. Pure states are also attractive due to their computational advantage in computing \cref{eq:triangle_quantum_distributions}. If each state $\rho$ is decomposable into $\ket{\psi}\bra{\psi}$, then \cref{eq:triangle_quantum_distributions} can be written as,
    \[ \prob[ABC]\br{abc} = \bramidket{\psi_{AB}\psi_{BC}\psi_{CA}}{\Omega M_{A,a}\otimes M_{B,b} \otimes M_{C,c} \Omega^\intercal}{\psi_{AB}\psi_{BC}\psi_{CA}} \eq \label{eq:pure_state_quantum_triangle} \]
    Avoiding the expensive matrix multiplications of \cref{eq:triangle_quantum_distributions}.
    \subsection{Measurements}
    With full generality, we consider a measurement $M$ to be a \term{projective-operator valued measure (POVM)} represented by a set of hermitian, positive semi-definite operators $\bc{M_i}_{i=1,\ldots,k}$\footnote{The $i$-th element of $M$ is referenced using a subscript $M_i$. The set of measurement elements for a particular party $X$ will be written $M_X$. When both party and element are to be referenced, we write $M_{X,i}$.} acting on $\Hilb^d$ summing to the identity,
    \[ \forall \ket{\phi} \in \Hilb^d : \bra{\phi} M_i \ket{\phi} \geq 0 \quad \sum_{i=1}^{k} M_i = \mathbb{I}_{\Hilb^d} \eq \label{eq:povm} \]
    When considering $k=2$ outcome measurements acting on $\Hilb^4$ we parameterize the first POVM element $M_1$ by using a Cholesky parameterization similar to \cref{eq:cholesky_param} without normalizing for trace. Afterwards, $M_2$ is fully determined by \cref{eq:povm}.
    \[ M_1 = T^\dagger T \quad M_2 = \mathbb{I} - T^\dagger T \eq \label{eq:2_povm}\]
    However, in order for $M_2$ to be positive semi-definite, the largest eigenvalue of $M_1$ has to be less than 1. To see this is a necessary and sufficient constraint, first expand out \cref{eq:2_povm},
    \[ \bramidket{\phi}{M_2}{\phi} = \bramidket{\phi}{\mathbb{I} - M_1}{\phi} = \abs{\phi}^2 - \bra{\phi}\br{\sum_{i=1}^{d}m^{\br{i}}_1 \ket{m^{\br{i}}_1}\bra{m^{\br{i}}_1}}\ket{\phi} \]
    Next write a generic $\ket{\phi} \in \Hilb^d$ in terms of a linear combination of the eigenvectors of $M_1$\footnote{The eigenvectors of $M_1$ form an orthonormal basis for $\Hilb^d$ because $M_1$ is Hermitian}.
    \[ \bramidket{\phi}{M_2}{\phi} = \sum_{j}\abs{\braket{\phi}{m^{\br{j}}_1}}^2 - \sum_{i} m^{\br{i}}_1 \abs{\braket{\phi}{m^{\br{i}}_1}}^2 = \sum_{i} \br{1 - m^{\br{i}}_1} \abs{\braket{\phi}{m^{\br{i}}_1}}^2 \eq \label{eq:eigen_less_one} \]
    Since $\ket{\phi}$ is arbitrary, for each $i$ set $\ket{\phi} = \ket{m^{\br{i}}_1}$ to see that each eigenvalue of $M_1$ needs to be less than $1$.
    \[ \bramidket{m^{\br{i}}_1}{M_2}{m^{\br{i}}_1} = \br{1 - m^{\br{i}}_1} \geq 0 \implies m^{\br{i}}_1 \leq 1 \eq \label{eq:2_povm_suff} \]
    By \cref{eq:eigen_less_one} is not difficult to see that \cref{eq:2_povm_suff} is a sufficient condition. During optimization, \cref{eq:2_povm_suff} can either be enforced passively as a constraint or directly by normalizing $M_1$ by its largest eigenvalue $\max\br{m_1^{\br{i}}}$ whenever necessary.

    When generalizing the above parameterization to more than $2$ outcomes, only necessary conditions were found. Generating $k$-outcome POVM measurements is doable using rejection sampling techniques such as those used in \cite{Petz_2012} however a valid parameterization with little to no degeneracy was not found. Upon making this observation, a necessary departure to \term{projective-valued measures (PVMs)} is warranted\footnote{Strictly speaking, when the number of outcomes ($k$) \textit{matches} the Hilbert space dimension ($d$), \cref{eq:povm} implies \cref{eq:pvm} by completeness. When considering $4$ outcome measurements on bipartite qubit states in $\Hilb^4$, PVMs are completely general. Moreover, Naimark's dilation theorem guarantees that PVMs acting on $\Hilb^q$ can \textit{emulate} the behaviour of any POVM acting on $\Hilb^d$ provided that $q$ is sufficiently larger than $d$ \cite{Naimark}.}. With loss of generality, consider the set of PVMs $M$ satisfying \cref{eq:povm} in addition to orthogonal and projective properties,
    \[ M_i M_j = \de_{ij} M_i \quad M_i = \ket{m_i}\bra{m_i} \eq \label{eq:pvm}\]
    Parameterizing $M$ for $k$-outcome measurements corresponds to parameterizing the set of all $k$-th order orthonormal sub-bases of $\Hilb^d$.
    First note that any such basis $\bc{\ket{\psi_1}, \ldots, \ket{\psi_k}}$ can be transformed into the computational basis $\bc{\ket{1}, \ldots, \ket{k}}$ by a unitary denoted $U \in \mathcal{U}\br{d}$,
    \[ U \ket{\psi_i} = \ket{i} \]
    With this observation we just need to parameterize the set of all unitaries $\mathcal{U}\br{d}$,
    \[ M = \bc{ U\ket{i}\bra{i}U^\dagger }_{i \in 1, \ldots, k} \]
    Specifically, the projective property each $M_i$ means that the global phase of $U$ is completely arbitrary; one only needs to consider parameterizing unitaries up to global phase \cref{eq:fast_spengler_unitary_gp}. This method was inspired by the \textit{measurement seeding} method of P{\'{a}}l and V{\'{e}}rtesi's \cite{Pal_2010} iterative optimization technique.

    Analogously to \cref{eq:pure_state_quantum_triangle}, projective measurements offer considerable computational advantage as \cref{eq:triangle_quantum_distributions} can be rewritten as,
    \[ \prob[ABC]\br{abc} = \bramidket{m_{A,a}m_{B,b}m_{C,c}}{\Omega^\intercal \rho_{AB}\otimes\rho_{BC}\otimes\rho_{CA} \Omega}{m_{A,a}m_{B,b}m_{C,c}} \eq \label{eq:pure_meas_quantum_triangle} \]

    \subsection{Network Permutation Matrix}
    \begin{figure}
        \centering
        \includegraphics[trim={1cm 1.2cm 1.0cm 1cm},clip,width=0.4\textwidth]{../figures/perm_mtrx.pdf}
        \caption{The network permutation matrix $\Omega$ for $\br{\Hilb^2}^{\otimes 6}$ realized on the triangle scenario. Black represents a value of $1$ and $0$ otherwise.}
        \label{fig:perm_mtrx}
    \end{figure}
    Finally, we introduce the \term{network permutation matrix} $\Omega$ for the Triangle Scenario of \cref{fig:triangle_scenario}. For bipartite qubit states, $\Omega$ becomes a $64\times64$ bit-wise matrix that acts on the measurements $M$ and is depicted in \cref{fig:perm_mtrx}. To illuminate its necessity, consider \cref{eq:triangle_quantum_distributions} without $\Omega$.
    \begin{align*}
    \prob[ABC]\br{abc} &\stackrel{?}{=} \Tr\bs{\br{\rho_{AB}\otimes\rho_{BC}\otimes\rho_{CA}} \br{M^a_{A}\otimes M^b_{B} \otimes M^c_{C}}}\\
    &= \Tr\bs{\br{\rho_{AB}M^a_{A}}\otimes\br{\rho_{BC}M^b_{B}}\otimes\br{\rho_{CA}M^c_{C}}}\\
    &= \Tr\br{\rho_{AB}M^a_{A}}\Tr{\br{\rho_{BC}M^b_{B}}}\Tr{\br{\rho_{CA}M^c_{C}}}\\
    &= \prob[A\mid \rho_{AB}]\br{a}\prob[B\mid \rho_{BC}]\br{b}\prob[C\mid \rho_{CA}]\br{c}
    \end{align*}
    On an operational level, this corresponds to $A$ making a measurement on \textit{both} subsystems of $\rho_{AB}$ and \textit{not} on any component of $\rho_{CA}$. This is analogously troubling for $B$ and $C$ as well. The network permutation matrix $\Omega$ corresponds to \textit{aligning} the underlying $6$-qubit joint state $\rho$ with the joint measurement $M$. To understand its effect, consider its effect on $6$-qubit pure state $\ket{q_1} \otimes \cdots \otimes \ket{q_6} = \ket{q_1q_2q_3q_4q_5q_6}$ where $\forall i : \ket{q_i} \in \Hilb^2$.
    \[ \Omega\ket{q_1q_2q_3q_4q_5q_6} = \ket{q_2q_3q_4q_5q_6q_1} \]
    $\Omega$ acts as a \textit{partial transpose} on $\br{\Hilb^2}^{\otimes 6}$ by shifting the underlying tensor structure one subsystem to the ``left''. It is uniquely defined by its action on all $2^6$ orthonormal basis elements of $\br{\Hilb^2}^{\otimes 6}$,
    \[ \Omega \defined \sum_{\ket{q_i} \in \bc{\ket{0}, \ket{1}}}\ket{q_2q_3q_4q_5q_6q_1}\bra{q_1q_2q_3q_4q_5q_6} \]
    \subsection{Degeneracy}
    \todo[TC]{Discuss local unitary degeneracy}
    \section{Convex Parametrization of Finite Probability Distributions}
    As discussed in \cref{sec:optimizations}, there is a need to parameterize the family of all probability distributions $\prob[V]$ over a given set of variables $V = \br{v_1, \ldots, v_{\abs{V}}}$. If the cardinality of $O_{V}$ is finite, then this computationally feasible. The space of probability distributions over $n = \abs{O_{V}}$ distinct outcomes forms a $n-1$ dimensional convex polytope naturally embedded in $\R_{\geq 0}^n$ \cite{Brunner_2013} that is parameterizable by $n-1$ real value parameters; normalization $\sum_{o\bs{V} \in O_V} \prob[V][\outc{V}] = 1$ accounts for the `$-1$'. An example of a non-degenerate parameterization of $\prob[V]$ consists of $n-1$ parameters $\lambda = \br{\lambda_1, \ldots, \lambda_{n-1}}, \lambda_i \in \bs{0, \pi/2}$ which generate the $n$ probability values $p_j$ using hyperspherical coordinates \cite{Hedemann_2013, Spengler_2010_Unitary},
    \begin{equation}
    \begin{gathered}
        \label{eq:convex_param}
        p_j = \cos^2 \lambda_j \prod_{i=1}^{j-1} \sin^2 \lambda_i \quad \forall j \in 1, \ldots, n - 1 \\
        p_n = \prod_{i=1}^{n-1} \sin^2 \lambda_i
    \end{gathered}
    \end{equation}
    Furthermore due to the periodicity of the parameter space $\lambda$, \cref{eq:convex_param} can be used for either constrained or unconstrained optimization problems. For continuity reasons, unconstrained optimizations are performed whenever possible.

    Although non-degenerate, this parameterization suffers from uniformity; a randomly sampled vector of parameters $\lambda$ \textit{does not} translate to a randomly sampled probability $\prob[V]$. An easy-to-implement, degenerate parameterization of $\prob[V]$ can be constructed by simply beginning with $n$ real parameters $\lambda = \br{\lambda_1, \ldots, \lambda_n}$, then making them positive and normalized by their sum\footnote{Strictly speaking, \cref{eq:uniform_param} \textit{also} suffers from non-uniformity; being biased toward uniform probability distributions $\prob[V]$. \todo[TC]{Discuss rejection sampling simplex algorithms}}.
    \[ p_j = \f{\abs{\lambda_j}}{\sum_{i=1}^{n} \abs{\lambda_i}} \quad \forall j \in 1, \ldots, n \eq \label{eq:uniform_param} \]
    For various convex optimization tasks sensitive to initial conditions outlined \cref{sec:optimizations}, the latter parameterization of \cref{eq:uniform_param} generally performed better than the former \cref{eq:convex_param}.

    \bibliography{references}
\end{document}